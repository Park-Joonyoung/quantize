{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1a842e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch = 2.8.0+cu128\n",
      "Trying PT2E prepare on nn.Module …\n",
      "Module path failed: AttributeError(\"'MobileNetV2' object has no attribute 'meta'\")\n",
      "Trying PT2E prepare on ExportedProgram …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75595/1373899070.py:31: DeprecationWarning: XNNPACKQuantizer is deprecated! Please use xnnpack quantizer in ExecuTorch (https://github.com/pytorch/executorch/tree/main/backends/xnnpack/quantizer) instead.\n",
      "  quantizer = XNNPACKQuantizer()\n",
      "/tmp/ipykernel_75595/1373899070.py:39: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(fp32_cpu, quantizer).eval()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram path failed: AttributeError(\"'ExportedProgram' object has no attribute 'meta'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75595/1373899070.py:48: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(exported, quantizer).eval()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "prepare_pt2e failed for both nn.Module and ExportedProgram variants. Consider upgrading PyTorch/torchvision.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrying PT2E prepare on nn.Module …\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m prepared = \u001b[43mprepare_pt2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp32_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m     40\u001b[39m used_variant = \u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3003\u001b[39m warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_pt2e.py:94\u001b[39m, in \u001b[36mprepare_pt2e\u001b[39m\u001b[34m(model, quantizer)\u001b[39m\n\u001b[32m     93\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_pt2e.prepare_pt2e\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m original_graph_meta = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeta\u001b[49m\n\u001b[32m     95\u001b[39m node_name_to_scope = _get_node_name_to_scope(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/nn/modules/module.py:1962\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1961\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1963\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1964\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'MobileNetV2' object has no attribute 'meta'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     47\u001b[39m exported = torch_export(fp32_cpu, ex_in)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m prepared = \u001b[43mprepare_pt2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexported\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m     49\u001b[39m used_variant = \u001b[33m\"\u001b[39m\u001b[33mexported\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3003\u001b[39m warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_pt2e.py:94\u001b[39m, in \u001b[36mprepare_pt2e\u001b[39m\u001b[34m(model, quantizer)\u001b[39m\n\u001b[32m     93\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_pt2e.prepare_pt2e\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m original_graph_meta = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeta\u001b[49m\n\u001b[32m     95\u001b[39m node_name_to_scope = _get_node_name_to_scope(model)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ExportedProgram' object has no attribute 'meta'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m         export_tflite(int8_obj,    \u001b[33m\"\u001b[39m\u001b[33mmobilenetv2_int8.tflite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e_exp:\n\u001b[32m     51\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExportedProgram path failed:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e_exp))\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     53\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprepare_pt2e failed for both nn.Module and ExportedProgram \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mvariants. Consider upgrading PyTorch/torchvision.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m         )\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Calibrate\u001b[39;00m\n\u001b[32m     58\u001b[39m calibrate_callable(prepared, batches=\u001b[32m128\u001b[39m, bs=\u001b[32m8\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: prepare_pt2e failed for both nn.Module and ExportedProgram variants. Consider upgrading PyTorch/torchvision."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.export import export as torch_export\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "from torch.ao.quantization.quantizer.xnnpack_quantizer import XNNPACKQuantizer\n",
    "import ai_edge_torch\n",
    "\n",
    "def build_fp32_cpu_model():\n",
    "    m = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).eval().cpu()\n",
    "    m = m.to(memory_format=torch.channels_last)\n",
    "    return m\n",
    "\n",
    "def export_tflite(model_or_exported, out_path, sample_shape=(1, 3, 224, 224)):\n",
    "    # ai_edge_torch needs CPU sample inputs as a TUPLE\n",
    "    sample_inputs = (torch.randn(*sample_shape),)\n",
    "    edge = ai_edge_torch.convert(model_or_exported, sample_inputs)\n",
    "    edge.export(out_path)\n",
    "    print(f\"✓ Wrote {out_path}\")\n",
    "\n",
    "def calibrate_callable(callable_model, batches=128, bs=8):\n",
    "    with torch.no_grad():\n",
    "        for _ in range(batches):\n",
    "            x = torch.randn(bs, 3, 224, 224).to(memory_format=torch.channels_last)\n",
    "            callable_model(x)\n",
    "\n",
    "def main():\n",
    "    print(\"torch =\", torch.__version__)\n",
    "    fp32_cpu = build_fp32_cpu_model()\n",
    "\n",
    "    # Build quantizer config\n",
    "    quantizer = XNNPACKQuantizer()\n",
    "\n",
    "    prepared = None\n",
    "    used_variant = None\n",
    "\n",
    "    # Variant A: prepare_pt2e expects an nn.Module\n",
    "    try:\n",
    "        print(\"Trying PT2E prepare on nn.Module …\")\n",
    "        prepared = prepare_pt2e(fp32_cpu, quantizer).eval()\n",
    "        used_variant = \"module\"\n",
    "    except Exception as e_mod:\n",
    "        print(\"Module path failed:\", repr(e_mod))\n",
    "        # Variant B: prepare_pt2e expects an ExportedProgram\n",
    "        try:\n",
    "            print(\"Trying PT2E prepare on ExportedProgram …\")\n",
    "            ex_in = (torch.randn(1, 3, 224, 224).to(memory_format=torch.channels_last),)\n",
    "            exported = torch_export(fp32_cpu, ex_in)\n",
    "            prepared = prepare_pt2e(exported, quantizer).eval()\n",
    "            used_variant = \"exported\"\n",
    "        except Exception as e_exp:\n",
    "            print(\"ExportedProgram path failed:\", repr(e_exp))\n",
    "            raise RuntimeError(\n",
    "                \"prepare_pt2e failed for both nn.Module and ExportedProgram \"\n",
    "                \"variants. Consider upgrading PyTorch/torchvision.\"\n",
    "            )\n",
    "\n",
    "    # Calibrate\n",
    "    calibrate_callable(prepared, batches=128, bs=8)\n",
    "\n",
    "    # Convert to INT8\n",
    "    int8_obj = convert_pt2e(prepared).eval()\n",
    "\n",
    "    # Export TFLite\n",
    "    if used_variant == \"module\":\n",
    "        # For consistency, we can also export FP32 from module:\n",
    "        export_tflite(fp32_cpu, \"mobilenetv2_fp32.tflite\")\n",
    "        export_tflite(int8_obj, \"mobilenetv2_int8.tflite\")\n",
    "    else:\n",
    "        # used_variant == \"exported\": prepared/converted are ExportedPrograms\n",
    "        # Also export FP32 as ExportedProgram so shapes/layouts match closely\n",
    "        ex_in = (torch.randn(1, 3, 224, 224).to(memory_format=torch.channels_last),)\n",
    "        exported_fp32 = torch_export(fp32_cpu, ex_in)\n",
    "        export_tflite(exported_fp32, \"mobilenetv2_fp32.tflite\")\n",
    "        export_tflite(int8_obj,    \"mobilenetv2_int8.tflite\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa6276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757083531.053137  107884 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "I0000 00:00:1757083531.592349  107884 cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757083534.301964  107884 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:351: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "WARNING:2025-09-05 23:45:36,251:jax._src.xla_bridge:864: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_107884/3848746058.py:41: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(m_fp32, quantizer, example_inputs=ex_in).eval()\n",
      "/tmp/ipykernel_107884/3848746058.py:45: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(m_fp32, quantizer).eval()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.0.dev20250811+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107884/3848746058.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(exported, quantizer).eval()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExportedProgram' object has no attribute 'meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     prepared = \u001b[43mprepare_pt2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_fp32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mex_in\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m     42\u001b[39m     used_variant = (\u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwith_example_inputs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3003\u001b[39m warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: prepare_pt2e() got an unexpected keyword argument 'example_inputs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     prepared = \u001b[43mprepare_pt2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_fp32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m     46\u001b[39m     used_variant = (\u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mno_example_inputs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3003\u001b[39m warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_pt2e.py:94\u001b[39m, in \u001b[36mprepare_pt2e\u001b[39m\u001b[34m(model, quantizer)\u001b[39m\n\u001b[32m     93\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_pt2e.prepare_pt2e\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m original_graph_meta = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeta\u001b[49m\n\u001b[32m     95\u001b[39m node_name_to_scope = _get_node_name_to_scope(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/nn/modules/module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'MobileNetV2' object has no attribute 'meta'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m         export_tflite(converted,      \u001b[33m\"\u001b[39m\u001b[33mmobilenetv2_int8.tflite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e_mod:\n\u001b[32m     48\u001b[39m         \u001b[38;5;66;03m# Fall back to ExportedProgram path\u001b[39;00m\n\u001b[32m     49\u001b[39m         exported = torch_export(m_fp32, ex_in)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         prepared = \u001b[43mprepare_pt2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexported\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m     51\u001b[39m         used_variant = (\u001b[33m\"\u001b[39m\u001b[33mexported_program\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mno_example_inputs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Calibrate & convert\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3001\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(arg)\n\u001b[32m   3002\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   3003\u001b[39m     warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_pt2e.py:94\u001b[39m, in \u001b[36mprepare_pt2e\u001b[39m\u001b[34m(model, quantizer)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prepare a model for post training quantization\u001b[39;00m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03m    # calibrate(m, sample_inference_data)\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_pt2e.prepare_pt2e\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m original_graph_meta = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeta\u001b[49m\n\u001b[32m     95\u001b[39m node_name_to_scope = _get_node_name_to_scope(model)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# TODO: check qconfig_mapping to make sure conv and bn are both configured\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# to be quantized before fusion\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# TODO: (maybe) rewrite this with subgraph_rewriter\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'ExportedProgram' object has no attribute 'meta'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "from torch.export import export as torch_export\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "\n",
    "# 👇 PyTorch 2.8 moved the quantizer here:\n",
    "from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import XNNPACKQuantizer\n",
    "\n",
    "import ai_edge_torch\n",
    "\n",
    "def export_tflite(model_or_exported, out_path, sample_shape=(1, 3, 224, 224)):\n",
    "    sample_inputs = (torch.randn(*sample_shape),)  # CPU tuple\n",
    "    edge = ai_edge_torch.convert(model_or_exported, sample_inputs)\n",
    "    edge.export(out_path)\n",
    "    print(f\"✓ Wrote {out_path}\")\n",
    "\n",
    "def build_fp32_cpu_model():\n",
    "    m = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).eval().cpu()\n",
    "    return m.to(memory_format=torch.channels_last)\n",
    "\n",
    "def calibrate_callable(callable_model, batches=128, bs=8):\n",
    "    with torch.no_grad():\n",
    "        for _ in range(batches):\n",
    "            x = torch.randn(bs, 3, 224, 224).to(memory_format=torch.channels_last)\n",
    "            callable_model(x)\n",
    "\n",
    "def main():\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    m_fp32 = build_fp32_cpu_model()\n",
    "\n",
    "    # Quantizer (ExecuTorch in 2.8)\n",
    "    quantizer = XNNPACKQuantizer()\n",
    "\n",
    "    prepared = None\n",
    "    used_variant = None\n",
    "    ex_in = (torch.randn(1, 3, 224, 224).to(memory_format=torch.channels_last),)\n",
    "\n",
    "    # Try Module path with/without example_inputs first\n",
    "    try:\n",
    "        prepared = prepare_pt2e(m_fp32, quantizer, example_inputs=ex_in).eval()\n",
    "        used_variant = (\"module\", \"with_example_inputs\")\n",
    "    except TypeError:\n",
    "        try:\n",
    "            prepared = prepare_pt2e(m_fp32, quantizer).eval()\n",
    "            used_variant = (\"module\", \"no_example_inputs\")\n",
    "        except Exception as e_mod:\n",
    "            # Fall back to ExportedProgram path\n",
    "            exported = torch_export(m_fp32, ex_in)\n",
    "            prepared = prepare_pt2e(exported, quantizer).eval()\n",
    "            used_variant = (\"exported_program\", \"no_example_inputs\")\n",
    "\n",
    "    # Calibrate & convert\n",
    "    calibrate_callable(prepared, batches=128, bs=8)\n",
    "    converted = convert_pt2e(prepared).eval()\n",
    "\n",
    "    # Export FP32 + INT8 TFLite\n",
    "    if used_variant[0] == \"module\":\n",
    "        export_tflite(m_fp32, \"mobilenetv2_fp32.tflite\")\n",
    "        export_tflite(converted, \"mobilenetv2_int8.tflite\")\n",
    "    else:\n",
    "        exported_fp32 = torch_export(m_fp32, ex_in)\n",
    "        export_tflite(exported_fp32, \"mobilenetv2_fp32.tflite\")\n",
    "        export_tflite(converted,      \"mobilenetv2_int8.tflite\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a86c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /home/joonyoung/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 18.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.exir import to_edge_transform_and_lower\n",
    "\n",
    "model = models.mobilenetv2.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).eval()\n",
    "sample_inputs = (torch.randn(1, 3, 224, 224), )\n",
    "\n",
    "et_program = to_edge_transform_and_lower(\n",
    "    torch.export.export(model, sample_inputs),\n",
    "    partitioner=[XnnpackPartitioner()]\n",
    ").to_executorch()\n",
    "\n",
    "with open(\"model.pte\", \"wb\") as f:\n",
    "    f.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd4b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:134] InternalConsistency verification requested but not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run succesfully via executorch\n",
      "Comparing against original PyTorch module\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from executorch.runtime import Runtime\n",
    "from typing import List\n",
    "\n",
    "runtime = Runtime.get()\n",
    "\n",
    "input_tensor: torch.Tensor = torch.randn(1, 3, 224, 224)\n",
    "program = runtime.load_program(\"model.pte\")\n",
    "method = program.load_method(\"forward\")\n",
    "output: List[torch.Tensor] = method.execute([input_tensor])\n",
    "print(\"Run succesfully via executorch\")\n",
    "\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "import torchvision.models as models\n",
    "\n",
    "eager_reference_model = models.mobilenetv2.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).eval()\n",
    "eager_reference_output = eager_reference_model(input_tensor)\n",
    "\n",
    "print(\"Comparing against original PyTorch module\")\n",
    "print(torch.allclose(output[0], eager_reference_output, rtol=1e-3, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2592f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6423/3929642179.py:27: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(gm, quantizer)\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
      "  aten_pattern = torch.export.export_for_training(\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
      "  aten_pattern = torch.export.export_for_training(\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
      "  aten_pattern = torch.export.export_for_training(\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
      "  aten_pattern = torch.export.export_for_training(\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
      "  aten_pattern = torch.export.export_for_training(\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/pt2e/utils.py:359: FutureWarning: `torch.export.export_for_training` is deprecated and will be removed in PyTorch 2.10. Please use `torch.export.export` instead, which is functionally equivalent.\n",
      "  aten_pattern = torch.export.export_for_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (features): Module(\n",
      "    (0): Module(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Module(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Module(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    features_0_0 = getattr(getattr(self.features, \"0\"), \"0\")(x);  x = None\n",
      "    features_0_1 = getattr(getattr(self.features, \"0\"), \"1\")(features_0_0);  features_0_0 = None\n",
      "    features_0_2 = getattr(getattr(self.features, \"0\"), \"2\")(features_0_1);  features_0_1 = None\n",
      "    features_1_conv_0_0 = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"0\")(features_0_2);  features_0_2 = None\n",
      "    features_1_conv_0_1 = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"1\")(features_1_conv_0_0);  features_1_conv_0_0 = None\n",
      "    features_1_conv_0_2 = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"2\")(features_1_conv_0_1);  features_1_conv_0_1 = None\n",
      "    features_1_conv_1 = getattr(getattr(self.features, \"1\").conv, \"1\")(features_1_conv_0_2);  features_1_conv_0_2 = None\n",
      "    features_1_conv_2 = getattr(getattr(self.features, \"1\").conv, \"2\")(features_1_conv_1);  features_1_conv_1 = None\n",
      "    features_2_conv_0_0 = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"0\")(features_1_conv_2);  features_1_conv_2 = None\n",
      "    features_2_conv_0_1 = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"1\")(features_2_conv_0_0);  features_2_conv_0_0 = None\n",
      "    features_2_conv_0_2 = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"2\")(features_2_conv_0_1);  features_2_conv_0_1 = None\n",
      "    features_2_conv_1_0 = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"0\")(features_2_conv_0_2);  features_2_conv_0_2 = None\n",
      "    features_2_conv_1_1 = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"1\")(features_2_conv_1_0);  features_2_conv_1_0 = None\n",
      "    features_2_conv_1_2 = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"2\")(features_2_conv_1_1);  features_2_conv_1_1 = None\n",
      "    features_2_conv_2 = getattr(getattr(self.features, \"2\").conv, \"2\")(features_2_conv_1_2);  features_2_conv_1_2 = None\n",
      "    features_2_conv_3 = getattr(getattr(self.features, \"2\").conv, \"3\")(features_2_conv_2);  features_2_conv_2 = None\n",
      "    features_3_conv_0_0 = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"0\")(features_2_conv_3)\n",
      "    features_3_conv_0_1 = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"1\")(features_3_conv_0_0);  features_3_conv_0_0 = None\n",
      "    features_3_conv_0_2 = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"2\")(features_3_conv_0_1);  features_3_conv_0_1 = None\n",
      "    features_3_conv_1_0 = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"0\")(features_3_conv_0_2);  features_3_conv_0_2 = None\n",
      "    features_3_conv_1_1 = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"1\")(features_3_conv_1_0);  features_3_conv_1_0 = None\n",
      "    features_3_conv_1_2 = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"2\")(features_3_conv_1_1);  features_3_conv_1_1 = None\n",
      "    features_3_conv_2 = getattr(getattr(self.features, \"3\").conv, \"2\")(features_3_conv_1_2);  features_3_conv_1_2 = None\n",
      "    features_3_conv_3 = getattr(getattr(self.features, \"3\").conv, \"3\")(features_3_conv_2);  features_3_conv_2 = None\n",
      "    add = features_2_conv_3 + features_3_conv_3;  features_2_conv_3 = features_3_conv_3 = None\n",
      "    features_4_conv_0_0 = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"0\")(add);  add = None\n",
      "    features_4_conv_0_1 = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"1\")(features_4_conv_0_0);  features_4_conv_0_0 = None\n",
      "    features_4_conv_0_2 = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"2\")(features_4_conv_0_1);  features_4_conv_0_1 = None\n",
      "    features_4_conv_1_0 = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"0\")(features_4_conv_0_2);  features_4_conv_0_2 = None\n",
      "    features_4_conv_1_1 = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"1\")(features_4_conv_1_0);  features_4_conv_1_0 = None\n",
      "    features_4_conv_1_2 = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"2\")(features_4_conv_1_1);  features_4_conv_1_1 = None\n",
      "    features_4_conv_2 = getattr(getattr(self.features, \"4\").conv, \"2\")(features_4_conv_1_2);  features_4_conv_1_2 = None\n",
      "    features_4_conv_3 = getattr(getattr(self.features, \"4\").conv, \"3\")(features_4_conv_2);  features_4_conv_2 = None\n",
      "    features_5_conv_0_0 = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"0\")(features_4_conv_3)\n",
      "    features_5_conv_0_1 = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"1\")(features_5_conv_0_0);  features_5_conv_0_0 = None\n",
      "    features_5_conv_0_2 = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"2\")(features_5_conv_0_1);  features_5_conv_0_1 = None\n",
      "    features_5_conv_1_0 = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"0\")(features_5_conv_0_2);  features_5_conv_0_2 = None\n",
      "    features_5_conv_1_1 = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"1\")(features_5_conv_1_0);  features_5_conv_1_0 = None\n",
      "    features_5_conv_1_2 = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"2\")(features_5_conv_1_1);  features_5_conv_1_1 = None\n",
      "    features_5_conv_2 = getattr(getattr(self.features, \"5\").conv, \"2\")(features_5_conv_1_2);  features_5_conv_1_2 = None\n",
      "    features_5_conv_3 = getattr(getattr(self.features, \"5\").conv, \"3\")(features_5_conv_2);  features_5_conv_2 = None\n",
      "    add_1 = features_4_conv_3 + features_5_conv_3;  features_4_conv_3 = features_5_conv_3 = None\n",
      "    features_6_conv_0_0 = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"0\")(add_1)\n",
      "    features_6_conv_0_1 = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"1\")(features_6_conv_0_0);  features_6_conv_0_0 = None\n",
      "    features_6_conv_0_2 = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"2\")(features_6_conv_0_1);  features_6_conv_0_1 = None\n",
      "    features_6_conv_1_0 = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"0\")(features_6_conv_0_2);  features_6_conv_0_2 = None\n",
      "    features_6_conv_1_1 = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"1\")(features_6_conv_1_0);  features_6_conv_1_0 = None\n",
      "    features_6_conv_1_2 = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"2\")(features_6_conv_1_1);  features_6_conv_1_1 = None\n",
      "    features_6_conv_2 = getattr(getattr(self.features, \"6\").conv, \"2\")(features_6_conv_1_2);  features_6_conv_1_2 = None\n",
      "    features_6_conv_3 = getattr(getattr(self.features, \"6\").conv, \"3\")(features_6_conv_2);  features_6_conv_2 = None\n",
      "    add_2 = add_1 + features_6_conv_3;  add_1 = features_6_conv_3 = None\n",
      "    features_7_conv_0_0 = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"0\")(add_2);  add_2 = None\n",
      "    features_7_conv_0_1 = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"1\")(features_7_conv_0_0);  features_7_conv_0_0 = None\n",
      "    features_7_conv_0_2 = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"2\")(features_7_conv_0_1);  features_7_conv_0_1 = None\n",
      "    features_7_conv_1_0 = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"0\")(features_7_conv_0_2);  features_7_conv_0_2 = None\n",
      "    features_7_conv_1_1 = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"1\")(features_7_conv_1_0);  features_7_conv_1_0 = None\n",
      "    features_7_conv_1_2 = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"2\")(features_7_conv_1_1);  features_7_conv_1_1 = None\n",
      "    features_7_conv_2 = getattr(getattr(self.features, \"7\").conv, \"2\")(features_7_conv_1_2);  features_7_conv_1_2 = None\n",
      "    features_7_conv_3 = getattr(getattr(self.features, \"7\").conv, \"3\")(features_7_conv_2);  features_7_conv_2 = None\n",
      "    features_8_conv_0_0 = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"0\")(features_7_conv_3)\n",
      "    features_8_conv_0_1 = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"1\")(features_8_conv_0_0);  features_8_conv_0_0 = None\n",
      "    features_8_conv_0_2 = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"2\")(features_8_conv_0_1);  features_8_conv_0_1 = None\n",
      "    features_8_conv_1_0 = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"0\")(features_8_conv_0_2);  features_8_conv_0_2 = None\n",
      "    features_8_conv_1_1 = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"1\")(features_8_conv_1_0);  features_8_conv_1_0 = None\n",
      "    features_8_conv_1_2 = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"2\")(features_8_conv_1_1);  features_8_conv_1_1 = None\n",
      "    features_8_conv_2 = getattr(getattr(self.features, \"8\").conv, \"2\")(features_8_conv_1_2);  features_8_conv_1_2 = None\n",
      "    features_8_conv_3 = getattr(getattr(self.features, \"8\").conv, \"3\")(features_8_conv_2);  features_8_conv_2 = None\n",
      "    add_3 = features_7_conv_3 + features_8_conv_3;  features_7_conv_3 = features_8_conv_3 = None\n",
      "    features_9_conv_0_0 = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"0\")(add_3)\n",
      "    features_9_conv_0_1 = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"1\")(features_9_conv_0_0);  features_9_conv_0_0 = None\n",
      "    features_9_conv_0_2 = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"2\")(features_9_conv_0_1);  features_9_conv_0_1 = None\n",
      "    features_9_conv_1_0 = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"0\")(features_9_conv_0_2);  features_9_conv_0_2 = None\n",
      "    features_9_conv_1_1 = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"1\")(features_9_conv_1_0);  features_9_conv_1_0 = None\n",
      "    features_9_conv_1_2 = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"2\")(features_9_conv_1_1);  features_9_conv_1_1 = None\n",
      "    features_9_conv_2 = getattr(getattr(self.features, \"9\").conv, \"2\")(features_9_conv_1_2);  features_9_conv_1_2 = None\n",
      "    features_9_conv_3 = getattr(getattr(self.features, \"9\").conv, \"3\")(features_9_conv_2);  features_9_conv_2 = None\n",
      "    add_4 = add_3 + features_9_conv_3;  add_3 = features_9_conv_3 = None\n",
      "    features_10_conv_0_0 = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"0\")(add_4)\n",
      "    features_10_conv_0_1 = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"1\")(features_10_conv_0_0);  features_10_conv_0_0 = None\n",
      "    features_10_conv_0_2 = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"2\")(features_10_conv_0_1);  features_10_conv_0_1 = None\n",
      "    features_10_conv_1_0 = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"0\")(features_10_conv_0_2);  features_10_conv_0_2 = None\n",
      "    features_10_conv_1_1 = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"1\")(features_10_conv_1_0);  features_10_conv_1_0 = None\n",
      "    features_10_conv_1_2 = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"2\")(features_10_conv_1_1);  features_10_conv_1_1 = None\n",
      "    features_10_conv_2 = getattr(getattr(self.features, \"10\").conv, \"2\")(features_10_conv_1_2);  features_10_conv_1_2 = None\n",
      "    features_10_conv_3 = getattr(getattr(self.features, \"10\").conv, \"3\")(features_10_conv_2);  features_10_conv_2 = None\n",
      "    add_5 = add_4 + features_10_conv_3;  add_4 = features_10_conv_3 = None\n",
      "    features_11_conv_0_0 = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"0\")(add_5);  add_5 = None\n",
      "    features_11_conv_0_1 = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"1\")(features_11_conv_0_0);  features_11_conv_0_0 = None\n",
      "    features_11_conv_0_2 = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"2\")(features_11_conv_0_1);  features_11_conv_0_1 = None\n",
      "    features_11_conv_1_0 = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"0\")(features_11_conv_0_2);  features_11_conv_0_2 = None\n",
      "    features_11_conv_1_1 = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"1\")(features_11_conv_1_0);  features_11_conv_1_0 = None\n",
      "    features_11_conv_1_2 = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"2\")(features_11_conv_1_1);  features_11_conv_1_1 = None\n",
      "    features_11_conv_2 = getattr(getattr(self.features, \"11\").conv, \"2\")(features_11_conv_1_2);  features_11_conv_1_2 = None\n",
      "    features_11_conv_3 = getattr(getattr(self.features, \"11\").conv, \"3\")(features_11_conv_2);  features_11_conv_2 = None\n",
      "    features_12_conv_0_0 = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"0\")(features_11_conv_3)\n",
      "    features_12_conv_0_1 = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"1\")(features_12_conv_0_0);  features_12_conv_0_0 = None\n",
      "    features_12_conv_0_2 = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"2\")(features_12_conv_0_1);  features_12_conv_0_1 = None\n",
      "    features_12_conv_1_0 = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"0\")(features_12_conv_0_2);  features_12_conv_0_2 = None\n",
      "    features_12_conv_1_1 = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"1\")(features_12_conv_1_0);  features_12_conv_1_0 = None\n",
      "    features_12_conv_1_2 = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"2\")(features_12_conv_1_1);  features_12_conv_1_1 = None\n",
      "    features_12_conv_2 = getattr(getattr(self.features, \"12\").conv, \"2\")(features_12_conv_1_2);  features_12_conv_1_2 = None\n",
      "    features_12_conv_3 = getattr(getattr(self.features, \"12\").conv, \"3\")(features_12_conv_2);  features_12_conv_2 = None\n",
      "    add_6 = features_11_conv_3 + features_12_conv_3;  features_11_conv_3 = features_12_conv_3 = None\n",
      "    features_13_conv_0_0 = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"0\")(add_6)\n",
      "    features_13_conv_0_1 = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"1\")(features_13_conv_0_0);  features_13_conv_0_0 = None\n",
      "    features_13_conv_0_2 = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"2\")(features_13_conv_0_1);  features_13_conv_0_1 = None\n",
      "    features_13_conv_1_0 = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"0\")(features_13_conv_0_2);  features_13_conv_0_2 = None\n",
      "    features_13_conv_1_1 = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"1\")(features_13_conv_1_0);  features_13_conv_1_0 = None\n",
      "    features_13_conv_1_2 = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"2\")(features_13_conv_1_1);  features_13_conv_1_1 = None\n",
      "    features_13_conv_2 = getattr(getattr(self.features, \"13\").conv, \"2\")(features_13_conv_1_2);  features_13_conv_1_2 = None\n",
      "    features_13_conv_3 = getattr(getattr(self.features, \"13\").conv, \"3\")(features_13_conv_2);  features_13_conv_2 = None\n",
      "    add_7 = add_6 + features_13_conv_3;  add_6 = features_13_conv_3 = None\n",
      "    features_14_conv_0_0 = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"0\")(add_7);  add_7 = None\n",
      "    features_14_conv_0_1 = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"1\")(features_14_conv_0_0);  features_14_conv_0_0 = None\n",
      "    features_14_conv_0_2 = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"2\")(features_14_conv_0_1);  features_14_conv_0_1 = None\n",
      "    features_14_conv_1_0 = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"0\")(features_14_conv_0_2);  features_14_conv_0_2 = None\n",
      "    features_14_conv_1_1 = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"1\")(features_14_conv_1_0);  features_14_conv_1_0 = None\n",
      "    features_14_conv_1_2 = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"2\")(features_14_conv_1_1);  features_14_conv_1_1 = None\n",
      "    features_14_conv_2 = getattr(getattr(self.features, \"14\").conv, \"2\")(features_14_conv_1_2);  features_14_conv_1_2 = None\n",
      "    features_14_conv_3 = getattr(getattr(self.features, \"14\").conv, \"3\")(features_14_conv_2);  features_14_conv_2 = None\n",
      "    features_15_conv_0_0 = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"0\")(features_14_conv_3)\n",
      "    features_15_conv_0_1 = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"1\")(features_15_conv_0_0);  features_15_conv_0_0 = None\n",
      "    features_15_conv_0_2 = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"2\")(features_15_conv_0_1);  features_15_conv_0_1 = None\n",
      "    features_15_conv_1_0 = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"0\")(features_15_conv_0_2);  features_15_conv_0_2 = None\n",
      "    features_15_conv_1_1 = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"1\")(features_15_conv_1_0);  features_15_conv_1_0 = None\n",
      "    features_15_conv_1_2 = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"2\")(features_15_conv_1_1);  features_15_conv_1_1 = None\n",
      "    features_15_conv_2 = getattr(getattr(self.features, \"15\").conv, \"2\")(features_15_conv_1_2);  features_15_conv_1_2 = None\n",
      "    features_15_conv_3 = getattr(getattr(self.features, \"15\").conv, \"3\")(features_15_conv_2);  features_15_conv_2 = None\n",
      "    add_8 = features_14_conv_3 + features_15_conv_3;  features_14_conv_3 = features_15_conv_3 = None\n",
      "    features_16_conv_0_0 = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"0\")(add_8)\n",
      "    features_16_conv_0_1 = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"1\")(features_16_conv_0_0);  features_16_conv_0_0 = None\n",
      "    features_16_conv_0_2 = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"2\")(features_16_conv_0_1);  features_16_conv_0_1 = None\n",
      "    features_16_conv_1_0 = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"0\")(features_16_conv_0_2);  features_16_conv_0_2 = None\n",
      "    features_16_conv_1_1 = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"1\")(features_16_conv_1_0);  features_16_conv_1_0 = None\n",
      "    features_16_conv_1_2 = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"2\")(features_16_conv_1_1);  features_16_conv_1_1 = None\n",
      "    features_16_conv_2 = getattr(getattr(self.features, \"16\").conv, \"2\")(features_16_conv_1_2);  features_16_conv_1_2 = None\n",
      "    features_16_conv_3 = getattr(getattr(self.features, \"16\").conv, \"3\")(features_16_conv_2);  features_16_conv_2 = None\n",
      "    add_9 = add_8 + features_16_conv_3;  add_8 = features_16_conv_3 = None\n",
      "    features_17_conv_0_0 = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"0\")(add_9);  add_9 = None\n",
      "    features_17_conv_0_1 = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"1\")(features_17_conv_0_0);  features_17_conv_0_0 = None\n",
      "    features_17_conv_0_2 = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"2\")(features_17_conv_0_1);  features_17_conv_0_1 = None\n",
      "    features_17_conv_1_0 = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"0\")(features_17_conv_0_2);  features_17_conv_0_2 = None\n",
      "    features_17_conv_1_1 = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"1\")(features_17_conv_1_0);  features_17_conv_1_0 = None\n",
      "    features_17_conv_1_2 = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"2\")(features_17_conv_1_1);  features_17_conv_1_1 = None\n",
      "    features_17_conv_2 = getattr(getattr(self.features, \"17\").conv, \"2\")(features_17_conv_1_2);  features_17_conv_1_2 = None\n",
      "    features_17_conv_3 = getattr(getattr(self.features, \"17\").conv, \"3\")(features_17_conv_2);  features_17_conv_2 = None\n",
      "    features_18_0 = getattr(getattr(self.features, \"18\"), \"0\")(features_17_conv_3);  features_17_conv_3 = None\n",
      "    features_18_1 = getattr(getattr(self.features, \"18\"), \"1\")(features_18_0);  features_18_0 = None\n",
      "    features_18_2 = getattr(getattr(self.features, \"18\"), \"2\")(features_18_1);  features_18_1 = None\n",
      "    adaptive_avg_pool2d = torch.nn.functional.adaptive_avg_pool2d(features_18_2, (1, 1));  features_18_2 = None\n",
      "    flatten = torch.flatten(adaptive_avg_pool2d, 1);  adaptive_avg_pool2d = None\n",
      "    classifier_0 = getattr(self.classifier, \"0\")(flatten);  flatten = None\n",
      "    classifier_1 = getattr(self.classifier, \"1\")(classifier_0);  classifier_0 = None\n",
      "    return classifier_1\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6423/3929642179.py:36: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized = convert_pt2e(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'qconfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(prepared)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 5) Convert to quantized graph (keep Q/DQ explicit for StableHLO/TFLite)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m quantized = \u001b[43mconvert_pt2e\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_reference_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfold_quantize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m)\u001b[49m.eval()\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# (Optional) Inspect that quant ops are present\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m quantized.graph.nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3001\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(arg)\n\u001b[32m   3002\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   3003\u001b[39m     warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_pt2e.py:245\u001b[39m, in \u001b[36mconvert_pt2e\u001b[39m\u001b[34m(model, use_reference_representation, fold_quantize)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    241\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnexpected argument type for `use_reference_representation`, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplease make sure you intend to pass argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_reference_representation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to convert_pt2e\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m     )\n\u001b[32m    244\u001b[39m original_graph_meta = model.meta\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m model = \u001b[43m_convert_to_reference_decomposed_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m model = _fold_conv_bn_qat(model)\n\u001b[32m    248\u001b[39m pm = PassManager([DuplicateDQPass()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_fx.py:730\u001b[39m, in \u001b[36m_convert_to_reference_decomposed_fx\u001b[39m\u001b[34m(graph_module, convert_custom_config, qconfig_mapping, backend_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Convert a calibrated or trained model to a reference quantized model, with\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03mdecomposed representation for quantized Tensor\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[33;03msee https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    725\u001b[39m \n\u001b[32m    726\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    727\u001b[39m torch._C._log_api_usage_once(\n\u001b[32m    728\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquantization_api.quantize_fx._convert_to_reference_decomposed_fx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    729\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/quantize_fx.py:550\u001b[39m, in \u001b[36m_convert_fx\u001b[39m\u001b[34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[39m\n\u001b[32m    543\u001b[39m preserved_attr_names = convert_custom_config.preserved_attributes\n\u001b[32m    544\u001b[39m preserved_attrs = {\n\u001b[32m    545\u001b[39m     attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr)\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)\n\u001b[32m    548\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m quantized = \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_standalone_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig_flag\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m attach_preserved_attrs_to_model(quantized, preserved_attrs)\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m quantized\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/fx/convert.py:1229\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[39m\n\u001b[32m   1224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1225\u001b[39m         type_before_parametrizations(mod) \u001b[38;5;129;01min\u001b[39;00m fused_module_classes\n\u001b[32m   1226\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod[\u001b[32m0\u001b[39m]) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m root_module_classes\n\u001b[32m   1227\u001b[39m     ):  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m   1228\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1229\u001b[39m     \u001b[43mconvert_weighted_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode_name_to_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01min\u001b[39;00m custom_module_classes:\n\u001b[32m   1239\u001b[39m     convert_custom_module(\n\u001b[32m   1240\u001b[39m         node,\n\u001b[32m   1241\u001b[39m         model.graph,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m         statically_quantized_custom_module_nodes,\n\u001b[32m   1245\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/ao/quantization/fx/convert.py:755\u001b[39m, in \u001b[36mconvert_weighted_module\u001b[39m\u001b[34m(node, modules, observed_node_names, node_name_to_qconfig, backend_config, is_decomposed, is_reference)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert a weighted module to reference quantized module in the model\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03mIf the QConfig of a QAT module is not set, the module will still be converted to\u001b[39;00m\n\u001b[32m    746\u001b[39m \u001b[33;03ma float module.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    752\u001b[39m \u001b[33;03m    this conversion if the node is not observed\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    754\u001b[39m original_module = modules[\u001b[38;5;28mstr\u001b[39m(node.target)]\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m qconfig: QConfigAny = \u001b[43moriginal_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqconfig\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    756\u001b[39m weight_post_process = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    757\u001b[39m qat_module_classes = get_qat_module_classes(backend_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/edge/lib/python3.11/site-packages/torch/nn/modules/module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Conv2d' object has no attribute 'qconfig'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "import torch.fx as fx\n",
    "\n",
    "# PT2E from torch.ao (not torchao)\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "from torch.ao.quantization import move_exported_model_to_eval\n",
    "\n",
    "# AI Edge Torch\n",
    "import ai_edge_torch as aet\n",
    "from ai_edge_torch.quantize import pt2e_quantizer as aet_q\n",
    "from ai_edge_torch.quantize import quant_config as aet_qc\n",
    "\n",
    "# 1) Load eager model + sample\n",
    "model = models.mobilenetv2.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).eval()\n",
    "sample_inputs = (torch.randn(1, 3, 224, 224),)\n",
    "\n",
    "# 2) Trace to FX for PT2E (we'll quantize THIS gm; do not use torch.export)\n",
    "gm = fx.symbolic_trace(model).eval()\n",
    "\n",
    "# 3) AET PT2E quantizer (TFLite-style: symmetric, per-channel weights on axis 0)\n",
    "qspec = aet_q.get_symmetric_quantization_config(is_per_channel=True)\n",
    "quantizer = aet_q.PT2EQuantizer().set_global(qspec)\n",
    "\n",
    "# 4) Prepare + calibrate on the FX GraphModule\n",
    "prepared = prepare_pt2e(gm, quantizer)\n",
    "prepared = move_exported_model_to_eval(prepared)\n",
    "with torch.no_grad():\n",
    "    for _ in range(32):                      # use real representative data if possible\n",
    "        prepared(torch.randn(1, 3, 224, 224))\n",
    "\n",
    "print(prepared)\n",
    "\n",
    "# 5) Convert to quantized graph (keep Q/DQ explicit for StableHLO/TFLite)\n",
    "quantized = convert_pt2e(\n",
    "    prepared,\n",
    "    use_reference_representation=False,\n",
    "    fold_quantize=False\n",
    ").eval()\n",
    "\n",
    "# (Optional) Inspect that quant ops are present\n",
    "for n in quantized.graph.nodes:\n",
    "    if \"quantize\" in str(n.target) or \"dequantize\" in str(n.target):\n",
    "        print(n.op, n.target)\n",
    "\n",
    "# 6) Export to TFLite via AI Edge Torch\n",
    "edge_model = aet.convert(\n",
    "    quantized,                      # NOTE: pass the quantized FX module (not EP)\n",
    "    sample_inputs,\n",
    "    quant_config=aet_qc.QuantConfig(pt2e_quantizer=quantizer),\n",
    ")\n",
    "edge_model.export(\"mobilenetv2_int8.tflite\")\n",
    "print(\"Wrote mobilenetv2_int8.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6423/138390372.py:25: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_pt2e(ep, quantizer)\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_1) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_2) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_3) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_4) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_5) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_6) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_7) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_8) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_9) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_10) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_11) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_12) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_13) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_14) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_15) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_16) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_17) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_18) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_19) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_20) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_21) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_22) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_23) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_24) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_25) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_26) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_27) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_28) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_29) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_30) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_31) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_32) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_33) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_34) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_35) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_36) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_37) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_38) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_39) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_40) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_41) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_42) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_43) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_44) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_45) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_46) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_47) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_48) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_49) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_50) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/home/joonyoung/.pyenv/versions/edge/lib/python3.11/site-packages/torch/fx/graph.py:1264: UserWarning: erase_node(batch_norm_51) on an already erased node\n",
      "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
      "/tmp/ipykernel_6423/138390372.py:30: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized = convert_pt2e(prepared, fold_quantize=False)\n",
      "WARNING:root:Your model is converted in training mode. Please set the module in evaluation mode with `module.eval()` for better on-device performance and compatibility.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (features): Module(\n",
      "    (0): Module(\n",
      "      (0): Module()\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module()\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (4): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (5): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (6): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (7): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (8): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (9): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (10): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (11): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (12): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (13): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (14): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (15): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (16): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (17): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): Module()\n",
      "        )\n",
      "        (2): Module()\n",
      "      )\n",
      "    )\n",
      "    (18): Module(\n",
      "      (0): Module()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Module(\n",
      "    (1): Module()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\n",
      "    features_0_0_weight = getattr(getattr(self.features, \"0\"), \"0\").weight\n",
      "    _scale_0 = self._scale_0\n",
      "    _zero_point_0 = self._zero_point_0\n",
      "    quantize_per_channel_default = torch.ops.quantized_decomposed.quantize_per_channel.default(features_0_0_weight, _scale_0, _zero_point_0, 0, -127, 127, torch.int8);  features_0_0_weight = None\n",
      "    dequantize_per_channel_default = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default, _scale_0, _zero_point_0, 0, -127, 127, torch.int8);  quantize_per_channel_default = _scale_0 = _zero_point_0 = None\n",
      "    features_1_conv_0_0_weight = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"0\").weight\n",
      "    _scale_1 = self._scale_1\n",
      "    _zero_point_1 = self._zero_point_1\n",
      "    quantize_per_channel_default_1 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_1_conv_0_0_weight, _scale_1, _zero_point_1, 0, -127, 127, torch.int8);  features_1_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_1 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_1, _scale_1, _zero_point_1, 0, -127, 127, torch.int8);  quantize_per_channel_default_1 = _scale_1 = _zero_point_1 = None\n",
      "    features_1_conv_1_weight = getattr(getattr(self.features, \"1\").conv, \"1\").weight\n",
      "    _scale_2 = self._scale_2\n",
      "    _zero_point_2 = self._zero_point_2\n",
      "    quantize_per_channel_default_2 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_1_conv_1_weight, _scale_2, _zero_point_2, 0, -127, 127, torch.int8);  features_1_conv_1_weight = None\n",
      "    dequantize_per_channel_default_2 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_2, _scale_2, _zero_point_2, 0, -127, 127, torch.int8);  quantize_per_channel_default_2 = _scale_2 = _zero_point_2 = None\n",
      "    features_2_conv_0_0_weight = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"0\").weight\n",
      "    _scale_3 = self._scale_3\n",
      "    _zero_point_3 = self._zero_point_3\n",
      "    quantize_per_channel_default_3 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_2_conv_0_0_weight, _scale_3, _zero_point_3, 0, -127, 127, torch.int8);  features_2_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_3 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_3, _scale_3, _zero_point_3, 0, -127, 127, torch.int8);  quantize_per_channel_default_3 = _scale_3 = _zero_point_3 = None\n",
      "    features_2_conv_1_0_weight = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"0\").weight\n",
      "    _scale_4 = self._scale_4\n",
      "    _zero_point_4 = self._zero_point_4\n",
      "    quantize_per_channel_default_4 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_2_conv_1_0_weight, _scale_4, _zero_point_4, 0, -127, 127, torch.int8);  features_2_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_4 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_4, _scale_4, _zero_point_4, 0, -127, 127, torch.int8);  quantize_per_channel_default_4 = _scale_4 = _zero_point_4 = None\n",
      "    features_2_conv_2_weight = getattr(getattr(self.features, \"2\").conv, \"2\").weight\n",
      "    _scale_5 = self._scale_5\n",
      "    _zero_point_5 = self._zero_point_5\n",
      "    quantize_per_channel_default_5 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_2_conv_2_weight, _scale_5, _zero_point_5, 0, -127, 127, torch.int8);  features_2_conv_2_weight = None\n",
      "    dequantize_per_channel_default_5 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_5, _scale_5, _zero_point_5, 0, -127, 127, torch.int8);  quantize_per_channel_default_5 = _scale_5 = _zero_point_5 = None\n",
      "    features_3_conv_0_0_weight = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"0\").weight\n",
      "    _scale_6 = self._scale_6\n",
      "    _zero_point_6 = self._zero_point_6\n",
      "    quantize_per_channel_default_6 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_3_conv_0_0_weight, _scale_6, _zero_point_6, 0, -127, 127, torch.int8);  features_3_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_6 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_6, _scale_6, _zero_point_6, 0, -127, 127, torch.int8);  quantize_per_channel_default_6 = _scale_6 = _zero_point_6 = None\n",
      "    features_3_conv_1_0_weight = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"0\").weight\n",
      "    _scale_7 = self._scale_7\n",
      "    _zero_point_7 = self._zero_point_7\n",
      "    quantize_per_channel_default_7 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_3_conv_1_0_weight, _scale_7, _zero_point_7, 0, -127, 127, torch.int8);  features_3_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_7 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_7, _scale_7, _zero_point_7, 0, -127, 127, torch.int8);  quantize_per_channel_default_7 = _scale_7 = _zero_point_7 = None\n",
      "    features_3_conv_2_weight = getattr(getattr(self.features, \"3\").conv, \"2\").weight\n",
      "    _scale_8 = self._scale_8\n",
      "    _zero_point_8 = self._zero_point_8\n",
      "    quantize_per_channel_default_8 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_3_conv_2_weight, _scale_8, _zero_point_8, 0, -127, 127, torch.int8);  features_3_conv_2_weight = None\n",
      "    dequantize_per_channel_default_8 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_8, _scale_8, _zero_point_8, 0, -127, 127, torch.int8);  quantize_per_channel_default_8 = _scale_8 = _zero_point_8 = None\n",
      "    features_4_conv_0_0_weight = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"0\").weight\n",
      "    _scale_9 = self._scale_9\n",
      "    _zero_point_9 = self._zero_point_9\n",
      "    quantize_per_channel_default_9 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_4_conv_0_0_weight, _scale_9, _zero_point_9, 0, -127, 127, torch.int8);  features_4_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_9 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_9, _scale_9, _zero_point_9, 0, -127, 127, torch.int8);  quantize_per_channel_default_9 = _scale_9 = _zero_point_9 = None\n",
      "    features_4_conv_1_0_weight = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"0\").weight\n",
      "    _scale_10 = self._scale_10\n",
      "    _zero_point_10 = self._zero_point_10\n",
      "    quantize_per_channel_default_10 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_4_conv_1_0_weight, _scale_10, _zero_point_10, 0, -127, 127, torch.int8);  features_4_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_10 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_10, _scale_10, _zero_point_10, 0, -127, 127, torch.int8);  quantize_per_channel_default_10 = _scale_10 = _zero_point_10 = None\n",
      "    features_4_conv_2_weight = getattr(getattr(self.features, \"4\").conv, \"2\").weight\n",
      "    _scale_11 = self._scale_11\n",
      "    _zero_point_11 = self._zero_point_11\n",
      "    quantize_per_channel_default_11 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_4_conv_2_weight, _scale_11, _zero_point_11, 0, -127, 127, torch.int8);  features_4_conv_2_weight = None\n",
      "    dequantize_per_channel_default_11 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_11, _scale_11, _zero_point_11, 0, -127, 127, torch.int8);  quantize_per_channel_default_11 = _scale_11 = _zero_point_11 = None\n",
      "    features_5_conv_0_0_weight = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"0\").weight\n",
      "    _scale_12 = self._scale_12\n",
      "    _zero_point_12 = self._zero_point_12\n",
      "    quantize_per_channel_default_12 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_5_conv_0_0_weight, _scale_12, _zero_point_12, 0, -127, 127, torch.int8);  features_5_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_12 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_12, _scale_12, _zero_point_12, 0, -127, 127, torch.int8);  quantize_per_channel_default_12 = _scale_12 = _zero_point_12 = None\n",
      "    features_5_conv_1_0_weight = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"0\").weight\n",
      "    _scale_13 = self._scale_13\n",
      "    _zero_point_13 = self._zero_point_13\n",
      "    quantize_per_channel_default_13 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_5_conv_1_0_weight, _scale_13, _zero_point_13, 0, -127, 127, torch.int8);  features_5_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_13 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_13, _scale_13, _zero_point_13, 0, -127, 127, torch.int8);  quantize_per_channel_default_13 = _scale_13 = _zero_point_13 = None\n",
      "    features_5_conv_2_weight = getattr(getattr(self.features, \"5\").conv, \"2\").weight\n",
      "    _scale_14 = self._scale_14\n",
      "    _zero_point_14 = self._zero_point_14\n",
      "    quantize_per_channel_default_14 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_5_conv_2_weight, _scale_14, _zero_point_14, 0, -127, 127, torch.int8);  features_5_conv_2_weight = None\n",
      "    dequantize_per_channel_default_14 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_14, _scale_14, _zero_point_14, 0, -127, 127, torch.int8);  quantize_per_channel_default_14 = _scale_14 = _zero_point_14 = None\n",
      "    features_6_conv_0_0_weight = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"0\").weight\n",
      "    _scale_15 = self._scale_15\n",
      "    _zero_point_15 = self._zero_point_15\n",
      "    quantize_per_channel_default_15 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_6_conv_0_0_weight, _scale_15, _zero_point_15, 0, -127, 127, torch.int8);  features_6_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_15 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_15, _scale_15, _zero_point_15, 0, -127, 127, torch.int8);  quantize_per_channel_default_15 = _scale_15 = _zero_point_15 = None\n",
      "    features_6_conv_1_0_weight = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"0\").weight\n",
      "    _scale_16 = self._scale_16\n",
      "    _zero_point_16 = self._zero_point_16\n",
      "    quantize_per_channel_default_16 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_6_conv_1_0_weight, _scale_16, _zero_point_16, 0, -127, 127, torch.int8);  features_6_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_16 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_16, _scale_16, _zero_point_16, 0, -127, 127, torch.int8);  quantize_per_channel_default_16 = _scale_16 = _zero_point_16 = None\n",
      "    features_6_conv_2_weight = getattr(getattr(self.features, \"6\").conv, \"2\").weight\n",
      "    _scale_17 = self._scale_17\n",
      "    _zero_point_17 = self._zero_point_17\n",
      "    quantize_per_channel_default_17 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_6_conv_2_weight, _scale_17, _zero_point_17, 0, -127, 127, torch.int8);  features_6_conv_2_weight = None\n",
      "    dequantize_per_channel_default_17 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_17, _scale_17, _zero_point_17, 0, -127, 127, torch.int8);  quantize_per_channel_default_17 = _scale_17 = _zero_point_17 = None\n",
      "    features_7_conv_0_0_weight = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"0\").weight\n",
      "    _scale_18 = self._scale_18\n",
      "    _zero_point_18 = self._zero_point_18\n",
      "    quantize_per_channel_default_18 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_7_conv_0_0_weight, _scale_18, _zero_point_18, 0, -127, 127, torch.int8);  features_7_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_18 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_18, _scale_18, _zero_point_18, 0, -127, 127, torch.int8);  quantize_per_channel_default_18 = _scale_18 = _zero_point_18 = None\n",
      "    features_7_conv_1_0_weight = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"0\").weight\n",
      "    _scale_19 = self._scale_19\n",
      "    _zero_point_19 = self._zero_point_19\n",
      "    quantize_per_channel_default_19 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_7_conv_1_0_weight, _scale_19, _zero_point_19, 0, -127, 127, torch.int8);  features_7_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_19 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_19, _scale_19, _zero_point_19, 0, -127, 127, torch.int8);  quantize_per_channel_default_19 = _scale_19 = _zero_point_19 = None\n",
      "    features_7_conv_2_weight = getattr(getattr(self.features, \"7\").conv, \"2\").weight\n",
      "    _scale_20 = self._scale_20\n",
      "    _zero_point_20 = self._zero_point_20\n",
      "    quantize_per_channel_default_20 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_7_conv_2_weight, _scale_20, _zero_point_20, 0, -127, 127, torch.int8);  features_7_conv_2_weight = None\n",
      "    dequantize_per_channel_default_20 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_20, _scale_20, _zero_point_20, 0, -127, 127, torch.int8);  quantize_per_channel_default_20 = _scale_20 = _zero_point_20 = None\n",
      "    features_8_conv_0_0_weight = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"0\").weight\n",
      "    _scale_21 = self._scale_21\n",
      "    _zero_point_21 = self._zero_point_21\n",
      "    quantize_per_channel_default_21 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_8_conv_0_0_weight, _scale_21, _zero_point_21, 0, -127, 127, torch.int8);  features_8_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_21 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_21, _scale_21, _zero_point_21, 0, -127, 127, torch.int8);  quantize_per_channel_default_21 = _scale_21 = _zero_point_21 = None\n",
      "    features_8_conv_1_0_weight = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"0\").weight\n",
      "    _scale_22 = self._scale_22\n",
      "    _zero_point_22 = self._zero_point_22\n",
      "    quantize_per_channel_default_22 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_8_conv_1_0_weight, _scale_22, _zero_point_22, 0, -127, 127, torch.int8);  features_8_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_22 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_22, _scale_22, _zero_point_22, 0, -127, 127, torch.int8);  quantize_per_channel_default_22 = _scale_22 = _zero_point_22 = None\n",
      "    features_8_conv_2_weight = getattr(getattr(self.features, \"8\").conv, \"2\").weight\n",
      "    _scale_23 = self._scale_23\n",
      "    _zero_point_23 = self._zero_point_23\n",
      "    quantize_per_channel_default_23 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_8_conv_2_weight, _scale_23, _zero_point_23, 0, -127, 127, torch.int8);  features_8_conv_2_weight = None\n",
      "    dequantize_per_channel_default_23 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_23, _scale_23, _zero_point_23, 0, -127, 127, torch.int8);  quantize_per_channel_default_23 = _scale_23 = _zero_point_23 = None\n",
      "    features_9_conv_0_0_weight = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"0\").weight\n",
      "    _scale_24 = self._scale_24\n",
      "    _zero_point_24 = self._zero_point_24\n",
      "    quantize_per_channel_default_24 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_9_conv_0_0_weight, _scale_24, _zero_point_24, 0, -127, 127, torch.int8);  features_9_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_24 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_24, _scale_24, _zero_point_24, 0, -127, 127, torch.int8);  quantize_per_channel_default_24 = _scale_24 = _zero_point_24 = None\n",
      "    features_9_conv_1_0_weight = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"0\").weight\n",
      "    _scale_25 = self._scale_25\n",
      "    _zero_point_25 = self._zero_point_25\n",
      "    quantize_per_channel_default_25 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_9_conv_1_0_weight, _scale_25, _zero_point_25, 0, -127, 127, torch.int8);  features_9_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_25 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_25, _scale_25, _zero_point_25, 0, -127, 127, torch.int8);  quantize_per_channel_default_25 = _scale_25 = _zero_point_25 = None\n",
      "    features_9_conv_2_weight = getattr(getattr(self.features, \"9\").conv, \"2\").weight\n",
      "    _scale_26 = self._scale_26\n",
      "    _zero_point_26 = self._zero_point_26\n",
      "    quantize_per_channel_default_26 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_9_conv_2_weight, _scale_26, _zero_point_26, 0, -127, 127, torch.int8);  features_9_conv_2_weight = None\n",
      "    dequantize_per_channel_default_26 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_26, _scale_26, _zero_point_26, 0, -127, 127, torch.int8);  quantize_per_channel_default_26 = _scale_26 = _zero_point_26 = None\n",
      "    features_10_conv_0_0_weight = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"0\").weight\n",
      "    _scale_27 = self._scale_27\n",
      "    _zero_point_27 = self._zero_point_27\n",
      "    quantize_per_channel_default_27 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_10_conv_0_0_weight, _scale_27, _zero_point_27, 0, -127, 127, torch.int8);  features_10_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_27 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_27, _scale_27, _zero_point_27, 0, -127, 127, torch.int8);  quantize_per_channel_default_27 = _scale_27 = _zero_point_27 = None\n",
      "    features_10_conv_1_0_weight = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"0\").weight\n",
      "    _scale_28 = self._scale_28\n",
      "    _zero_point_28 = self._zero_point_28\n",
      "    quantize_per_channel_default_28 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_10_conv_1_0_weight, _scale_28, _zero_point_28, 0, -127, 127, torch.int8);  features_10_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_28 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_28, _scale_28, _zero_point_28, 0, -127, 127, torch.int8);  quantize_per_channel_default_28 = _scale_28 = _zero_point_28 = None\n",
      "    features_10_conv_2_weight = getattr(getattr(self.features, \"10\").conv, \"2\").weight\n",
      "    _scale_29 = self._scale_29\n",
      "    _zero_point_29 = self._zero_point_29\n",
      "    quantize_per_channel_default_29 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_10_conv_2_weight, _scale_29, _zero_point_29, 0, -127, 127, torch.int8);  features_10_conv_2_weight = None\n",
      "    dequantize_per_channel_default_29 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_29, _scale_29, _zero_point_29, 0, -127, 127, torch.int8);  quantize_per_channel_default_29 = _scale_29 = _zero_point_29 = None\n",
      "    features_11_conv_0_0_weight = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"0\").weight\n",
      "    _scale_30 = self._scale_30\n",
      "    _zero_point_30 = self._zero_point_30\n",
      "    quantize_per_channel_default_30 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_11_conv_0_0_weight, _scale_30, _zero_point_30, 0, -127, 127, torch.int8);  features_11_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_30 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_30, _scale_30, _zero_point_30, 0, -127, 127, torch.int8);  quantize_per_channel_default_30 = _scale_30 = _zero_point_30 = None\n",
      "    features_11_conv_1_0_weight = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"0\").weight\n",
      "    _scale_31 = self._scale_31\n",
      "    _zero_point_31 = self._zero_point_31\n",
      "    quantize_per_channel_default_31 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_11_conv_1_0_weight, _scale_31, _zero_point_31, 0, -127, 127, torch.int8);  features_11_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_31 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_31, _scale_31, _zero_point_31, 0, -127, 127, torch.int8);  quantize_per_channel_default_31 = _scale_31 = _zero_point_31 = None\n",
      "    features_11_conv_2_weight = getattr(getattr(self.features, \"11\").conv, \"2\").weight\n",
      "    _scale_32 = self._scale_32\n",
      "    _zero_point_32 = self._zero_point_32\n",
      "    quantize_per_channel_default_32 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_11_conv_2_weight, _scale_32, _zero_point_32, 0, -127, 127, torch.int8);  features_11_conv_2_weight = None\n",
      "    dequantize_per_channel_default_32 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_32, _scale_32, _zero_point_32, 0, -127, 127, torch.int8);  quantize_per_channel_default_32 = _scale_32 = _zero_point_32 = None\n",
      "    features_12_conv_0_0_weight = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"0\").weight\n",
      "    _scale_33 = self._scale_33\n",
      "    _zero_point_33 = self._zero_point_33\n",
      "    quantize_per_channel_default_33 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_12_conv_0_0_weight, _scale_33, _zero_point_33, 0, -127, 127, torch.int8);  features_12_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_33 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_33, _scale_33, _zero_point_33, 0, -127, 127, torch.int8);  quantize_per_channel_default_33 = _scale_33 = _zero_point_33 = None\n",
      "    features_12_conv_1_0_weight = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"0\").weight\n",
      "    _scale_34 = self._scale_34\n",
      "    _zero_point_34 = self._zero_point_34\n",
      "    quantize_per_channel_default_34 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_12_conv_1_0_weight, _scale_34, _zero_point_34, 0, -127, 127, torch.int8);  features_12_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_34 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_34, _scale_34, _zero_point_34, 0, -127, 127, torch.int8);  quantize_per_channel_default_34 = _scale_34 = _zero_point_34 = None\n",
      "    features_12_conv_2_weight = getattr(getattr(self.features, \"12\").conv, \"2\").weight\n",
      "    _scale_35 = self._scale_35\n",
      "    _zero_point_35 = self._zero_point_35\n",
      "    quantize_per_channel_default_35 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_12_conv_2_weight, _scale_35, _zero_point_35, 0, -127, 127, torch.int8);  features_12_conv_2_weight = None\n",
      "    dequantize_per_channel_default_35 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_35, _scale_35, _zero_point_35, 0, -127, 127, torch.int8);  quantize_per_channel_default_35 = _scale_35 = _zero_point_35 = None\n",
      "    features_13_conv_0_0_weight = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"0\").weight\n",
      "    _scale_36 = self._scale_36\n",
      "    _zero_point_36 = self._zero_point_36\n",
      "    quantize_per_channel_default_36 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_13_conv_0_0_weight, _scale_36, _zero_point_36, 0, -127, 127, torch.int8);  features_13_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_36 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_36, _scale_36, _zero_point_36, 0, -127, 127, torch.int8);  quantize_per_channel_default_36 = _scale_36 = _zero_point_36 = None\n",
      "    features_13_conv_1_0_weight = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"0\").weight\n",
      "    _scale_37 = self._scale_37\n",
      "    _zero_point_37 = self._zero_point_37\n",
      "    quantize_per_channel_default_37 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_13_conv_1_0_weight, _scale_37, _zero_point_37, 0, -127, 127, torch.int8);  features_13_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_37 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_37, _scale_37, _zero_point_37, 0, -127, 127, torch.int8);  quantize_per_channel_default_37 = _scale_37 = _zero_point_37 = None\n",
      "    features_13_conv_2_weight = getattr(getattr(self.features, \"13\").conv, \"2\").weight\n",
      "    _scale_38 = self._scale_38\n",
      "    _zero_point_38 = self._zero_point_38\n",
      "    quantize_per_channel_default_38 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_13_conv_2_weight, _scale_38, _zero_point_38, 0, -127, 127, torch.int8);  features_13_conv_2_weight = None\n",
      "    dequantize_per_channel_default_38 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_38, _scale_38, _zero_point_38, 0, -127, 127, torch.int8);  quantize_per_channel_default_38 = _scale_38 = _zero_point_38 = None\n",
      "    features_14_conv_0_0_weight = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"0\").weight\n",
      "    _scale_39 = self._scale_39\n",
      "    _zero_point_39 = self._zero_point_39\n",
      "    quantize_per_channel_default_39 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_14_conv_0_0_weight, _scale_39, _zero_point_39, 0, -127, 127, torch.int8);  features_14_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_39 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_39, _scale_39, _zero_point_39, 0, -127, 127, torch.int8);  quantize_per_channel_default_39 = _scale_39 = _zero_point_39 = None\n",
      "    features_14_conv_1_0_weight = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"0\").weight\n",
      "    _scale_40 = self._scale_40\n",
      "    _zero_point_40 = self._zero_point_40\n",
      "    quantize_per_channel_default_40 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_14_conv_1_0_weight, _scale_40, _zero_point_40, 0, -127, 127, torch.int8);  features_14_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_40 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_40, _scale_40, _zero_point_40, 0, -127, 127, torch.int8);  quantize_per_channel_default_40 = _scale_40 = _zero_point_40 = None\n",
      "    features_14_conv_2_weight = getattr(getattr(self.features, \"14\").conv, \"2\").weight\n",
      "    _scale_41 = self._scale_41\n",
      "    _zero_point_41 = self._zero_point_41\n",
      "    quantize_per_channel_default_41 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_14_conv_2_weight, _scale_41, _zero_point_41, 0, -127, 127, torch.int8);  features_14_conv_2_weight = None\n",
      "    dequantize_per_channel_default_41 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_41, _scale_41, _zero_point_41, 0, -127, 127, torch.int8);  quantize_per_channel_default_41 = _scale_41 = _zero_point_41 = None\n",
      "    features_15_conv_0_0_weight = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"0\").weight\n",
      "    _scale_42 = self._scale_42\n",
      "    _zero_point_42 = self._zero_point_42\n",
      "    quantize_per_channel_default_42 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_15_conv_0_0_weight, _scale_42, _zero_point_42, 0, -127, 127, torch.int8);  features_15_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_42 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_42, _scale_42, _zero_point_42, 0, -127, 127, torch.int8);  quantize_per_channel_default_42 = _scale_42 = _zero_point_42 = None\n",
      "    features_15_conv_1_0_weight = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"0\").weight\n",
      "    _scale_43 = self._scale_43\n",
      "    _zero_point_43 = self._zero_point_43\n",
      "    quantize_per_channel_default_43 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_15_conv_1_0_weight, _scale_43, _zero_point_43, 0, -127, 127, torch.int8);  features_15_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_43 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_43, _scale_43, _zero_point_43, 0, -127, 127, torch.int8);  quantize_per_channel_default_43 = _scale_43 = _zero_point_43 = None\n",
      "    features_15_conv_2_weight = getattr(getattr(self.features, \"15\").conv, \"2\").weight\n",
      "    _scale_44 = self._scale_44\n",
      "    _zero_point_44 = self._zero_point_44\n",
      "    quantize_per_channel_default_44 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_15_conv_2_weight, _scale_44, _zero_point_44, 0, -127, 127, torch.int8);  features_15_conv_2_weight = None\n",
      "    dequantize_per_channel_default_44 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_44, _scale_44, _zero_point_44, 0, -127, 127, torch.int8);  quantize_per_channel_default_44 = _scale_44 = _zero_point_44 = None\n",
      "    features_16_conv_0_0_weight = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"0\").weight\n",
      "    _scale_45 = self._scale_45\n",
      "    _zero_point_45 = self._zero_point_45\n",
      "    quantize_per_channel_default_45 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_16_conv_0_0_weight, _scale_45, _zero_point_45, 0, -127, 127, torch.int8);  features_16_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_45 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_45, _scale_45, _zero_point_45, 0, -127, 127, torch.int8);  quantize_per_channel_default_45 = _scale_45 = _zero_point_45 = None\n",
      "    features_16_conv_1_0_weight = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"0\").weight\n",
      "    _scale_46 = self._scale_46\n",
      "    _zero_point_46 = self._zero_point_46\n",
      "    quantize_per_channel_default_46 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_16_conv_1_0_weight, _scale_46, _zero_point_46, 0, -127, 127, torch.int8);  features_16_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_46 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_46, _scale_46, _zero_point_46, 0, -127, 127, torch.int8);  quantize_per_channel_default_46 = _scale_46 = _zero_point_46 = None\n",
      "    features_16_conv_2_weight = getattr(getattr(self.features, \"16\").conv, \"2\").weight\n",
      "    _scale_47 = self._scale_47\n",
      "    _zero_point_47 = self._zero_point_47\n",
      "    quantize_per_channel_default_47 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_16_conv_2_weight, _scale_47, _zero_point_47, 0, -127, 127, torch.int8);  features_16_conv_2_weight = None\n",
      "    dequantize_per_channel_default_47 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_47, _scale_47, _zero_point_47, 0, -127, 127, torch.int8);  quantize_per_channel_default_47 = _scale_47 = _zero_point_47 = None\n",
      "    features_17_conv_0_0_weight = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"0\").weight\n",
      "    _scale_48 = self._scale_48\n",
      "    _zero_point_48 = self._zero_point_48\n",
      "    quantize_per_channel_default_48 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_17_conv_0_0_weight, _scale_48, _zero_point_48, 0, -127, 127, torch.int8);  features_17_conv_0_0_weight = None\n",
      "    dequantize_per_channel_default_48 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_48, _scale_48, _zero_point_48, 0, -127, 127, torch.int8);  quantize_per_channel_default_48 = _scale_48 = _zero_point_48 = None\n",
      "    features_17_conv_1_0_weight = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"0\").weight\n",
      "    _scale_49 = self._scale_49\n",
      "    _zero_point_49 = self._zero_point_49\n",
      "    quantize_per_channel_default_49 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_17_conv_1_0_weight, _scale_49, _zero_point_49, 0, -127, 127, torch.int8);  features_17_conv_1_0_weight = None\n",
      "    dequantize_per_channel_default_49 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_49, _scale_49, _zero_point_49, 0, -127, 127, torch.int8);  quantize_per_channel_default_49 = _scale_49 = _zero_point_49 = None\n",
      "    features_17_conv_2_weight = getattr(getattr(self.features, \"17\").conv, \"2\").weight\n",
      "    _scale_50 = self._scale_50\n",
      "    _zero_point_50 = self._zero_point_50\n",
      "    quantize_per_channel_default_50 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_17_conv_2_weight, _scale_50, _zero_point_50, 0, -127, 127, torch.int8);  features_17_conv_2_weight = None\n",
      "    dequantize_per_channel_default_50 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_50, _scale_50, _zero_point_50, 0, -127, 127, torch.int8);  quantize_per_channel_default_50 = _scale_50 = _zero_point_50 = None\n",
      "    features_18_0_weight = getattr(getattr(self.features, \"18\"), \"0\").weight\n",
      "    _scale_51 = self._scale_51\n",
      "    _zero_point_51 = self._zero_point_51\n",
      "    quantize_per_channel_default_51 = torch.ops.quantized_decomposed.quantize_per_channel.default(features_18_0_weight, _scale_51, _zero_point_51, 0, -127, 127, torch.int8);  features_18_0_weight = None\n",
      "    dequantize_per_channel_default_51 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_51, _scale_51, _zero_point_51, 0, -127, 127, torch.int8);  quantize_per_channel_default_51 = _scale_51 = _zero_point_51 = None\n",
      "    classifier_1_weight = getattr(self.classifier, \"1\").weight\n",
      "    _scale_52 = self._scale_52\n",
      "    _zero_point_52 = self._zero_point_52\n",
      "    quantize_per_channel_default_52 = torch.ops.quantized_decomposed.quantize_per_channel.default(classifier_1_weight, _scale_52, _zero_point_52, 0, -127, 127, torch.int8);  classifier_1_weight = None\n",
      "    dequantize_per_channel_default_52 = torch.ops.quantized_decomposed.dequantize_per_channel.default(quantize_per_channel_default_52, _scale_52, _zero_point_52, 0, -127, 127, torch.int8);  quantize_per_channel_default_52 = _scale_52 = _zero_point_52 = None\n",
      "    classifier_1_bias = getattr(self.classifier, \"1\").bias\n",
      "    quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(x, 0.030688544735312462, -1, -128, 127, torch.int8);  x = None\n",
      "    dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 0.030688544735312462, -1, -128, 127, torch.int8);  quantize_per_tensor_default = None\n",
      "    features_0_0_weight_bias = getattr(getattr(self.features, \"0\"), \"0\").weight_bias\n",
      "    conv2d = torch.ops.aten.conv2d.default(dequantize_per_tensor_default, dequantize_per_channel_default, features_0_0_weight_bias, [2, 2], [1, 1]);  dequantize_per_tensor_default = dequantize_per_channel_default = features_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d, 0.25043806433677673, -11, -128, 127, torch.int8);  conv2d = None\n",
      "    dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 0.25043806433677673, -11, -128, 127, torch.int8);  quantize_per_tensor_default_1 = None\n",
      "    hardtanh_ = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_1, 0.0, 6.0);  dequantize_per_tensor_default_1 = None\n",
      "    quantize_per_tensor_default_2 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh_, 0.25043806433677673, -11, -128, 127, torch.int8);  hardtanh_ = None\n",
      "    dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 0.25043806433677673, -11, -128, 127, torch.int8);  quantize_per_tensor_default_2 = None\n",
      "    features_1_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_1 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_2, dequantize_per_channel_default_1, features_1_conv_0_0_weight_bias, [1, 1], [1, 1], [1, 1], 32);  dequantize_per_tensor_default_2 = dequantize_per_channel_default_1 = features_1_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_3 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_1, 0.3814573287963867, -8, -128, 127, torch.int8);  conv2d_1 = None\n",
      "    dequantize_per_tensor_default_3 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_3, 0.3814573287963867, -8, -128, 127, torch.int8);  quantize_per_tensor_default_3 = None\n",
      "    hardtanh__1 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_3, 0.0, 6.0);  dequantize_per_tensor_default_3 = None\n",
      "    quantize_per_tensor_default_4 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__1, 0.3814573287963867, -8, -128, 127, torch.int8);  hardtanh__1 = None\n",
      "    dequantize_per_tensor_default_4 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_4, 0.3814573287963867, -8, -128, 127, torch.int8);  quantize_per_tensor_default_4 = None\n",
      "    features_1_conv_1_weight_bias = getattr(getattr(self.features, \"1\").conv, \"1\").weight_bias\n",
      "    conv2d_2 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_4, dequantize_per_channel_default_2, features_1_conv_1_weight_bias);  dequantize_per_tensor_default_4 = dequantize_per_channel_default_2 = features_1_conv_1_weight_bias = None\n",
      "    quantize_per_tensor_default_5 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_2, 0.5124295949935913, 20, -128, 127, torch.int8);  conv2d_2 = None\n",
      "    dequantize_per_tensor_default_5 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_5, 0.5124295949935913, 20, -128, 127, torch.int8);  quantize_per_tensor_default_5 = None\n",
      "    features_2_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_3 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_5, dequantize_per_channel_default_3, features_2_conv_0_0_weight_bias);  dequantize_per_tensor_default_5 = dequantize_per_channel_default_3 = features_2_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_6 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_3, 0.3699303865432739, -11, -128, 127, torch.int8);  conv2d_3 = None\n",
      "    dequantize_per_tensor_default_6 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_6, 0.3699303865432739, -11, -128, 127, torch.int8);  quantize_per_tensor_default_6 = None\n",
      "    hardtanh__2 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_6, 0.0, 6.0);  dequantize_per_tensor_default_6 = None\n",
      "    quantize_per_tensor_default_7 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__2, 0.3699303865432739, -11, -128, 127, torch.int8);  hardtanh__2 = None\n",
      "    dequantize_per_tensor_default_7 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_7, 0.3699303865432739, -11, -128, 127, torch.int8);  quantize_per_tensor_default_7 = None\n",
      "    features_2_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_4 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_7, dequantize_per_channel_default_4, features_2_conv_1_0_weight_bias, [2, 2], [1, 1], [1, 1], 96);  dequantize_per_tensor_default_7 = dequantize_per_channel_default_4 = features_2_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_8 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_4, 0.11371229588985443, 28, -128, 127, torch.int8);  conv2d_4 = None\n",
      "    dequantize_per_tensor_default_8 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_8, 0.11371229588985443, 28, -128, 127, torch.int8);  quantize_per_tensor_default_8 = None\n",
      "    hardtanh__3 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_8, 0.0, 6.0);  dequantize_per_tensor_default_8 = None\n",
      "    quantize_per_tensor_default_9 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__3, 0.11371229588985443, 28, -128, 127, torch.int8);  hardtanh__3 = None\n",
      "    dequantize_per_tensor_default_9 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_9, 0.11371229588985443, 28, -128, 127, torch.int8);  quantize_per_tensor_default_9 = None\n",
      "    features_2_conv_2_weight_bias = getattr(getattr(self.features, \"2\").conv, \"2\").weight_bias\n",
      "    conv2d_5 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_9, dequantize_per_channel_default_5, features_2_conv_2_weight_bias);  dequantize_per_tensor_default_9 = dequantize_per_channel_default_5 = features_2_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_10 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_5, 0.32883355021476746, 2, -128, 127, torch.int8);  conv2d_5 = None\n",
      "    dequantize_per_tensor_default_10 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_10, 0.32883355021476746, 2, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_102 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_10, 0.32883355021476746, 2, -128, 127, torch.int8);  quantize_per_tensor_default_10 = None\n",
      "    features_3_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_6 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_102, dequantize_per_channel_default_6, features_3_conv_0_0_weight_bias);  dequantize_per_tensor_default_102 = dequantize_per_channel_default_6 = features_3_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_11 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_6, 0.1671539694070816, 6, -128, 127, torch.int8);  conv2d_6 = None\n",
      "    dequantize_per_tensor_default_11 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_11, 0.1671539694070816, 6, -128, 127, torch.int8);  quantize_per_tensor_default_11 = None\n",
      "    hardtanh__4 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_11, 0.0, 6.0);  dequantize_per_tensor_default_11 = None\n",
      "    quantize_per_tensor_default_12 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__4, 0.1671539694070816, 6, -128, 127, torch.int8);  hardtanh__4 = None\n",
      "    dequantize_per_tensor_default_12 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_12, 0.1671539694070816, 6, -128, 127, torch.int8);  quantize_per_tensor_default_12 = None\n",
      "    features_3_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_7 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_12, dequantize_per_channel_default_7, features_3_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 144);  dequantize_per_tensor_default_12 = dequantize_per_channel_default_7 = features_3_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_13 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_7, 0.1579434871673584, 40, -128, 127, torch.int8);  conv2d_7 = None\n",
      "    dequantize_per_tensor_default_13 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_13, 0.1579434871673584, 40, -128, 127, torch.int8);  quantize_per_tensor_default_13 = None\n",
      "    hardtanh__5 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_13, 0.0, 6.0);  dequantize_per_tensor_default_13 = None\n",
      "    quantize_per_tensor_default_14 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__5, 0.1579434871673584, 40, -128, 127, torch.int8);  hardtanh__5 = None\n",
      "    dequantize_per_tensor_default_14 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_14, 0.1579434871673584, 40, -128, 127, torch.int8);  quantize_per_tensor_default_14 = None\n",
      "    features_3_conv_2_weight_bias = getattr(getattr(self.features, \"3\").conv, \"2\").weight_bias\n",
      "    conv2d_8 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_14, dequantize_per_channel_default_8, features_3_conv_2_weight_bias);  dequantize_per_tensor_default_14 = dequantize_per_channel_default_8 = features_3_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_15 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_8, 0.33807557821273804, -1, -128, 127, torch.int8);  conv2d_8 = None\n",
      "    dequantize_per_tensor_default_15 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_15, 0.33807557821273804, -1, -128, 127, torch.int8);  quantize_per_tensor_default_15 = None\n",
      "    add = torch.ops.aten.add.Tensor(dequantize_per_tensor_default_10, dequantize_per_tensor_default_15);  dequantize_per_tensor_default_10 = dequantize_per_tensor_default_15 = None\n",
      "    quantize_per_tensor_default_16 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add, 0.34901484847068787, 4, -128, 127, torch.int8);  add = None\n",
      "    dequantize_per_tensor_default_16 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_16, 0.34901484847068787, 4, -128, 127, torch.int8);  quantize_per_tensor_default_16 = None\n",
      "    features_4_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_9 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_16, dequantize_per_channel_default_9, features_4_conv_0_0_weight_bias);  dequantize_per_tensor_default_16 = dequantize_per_channel_default_9 = features_4_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_17 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_9, 0.17823737859725952, 0, -128, 127, torch.int8);  conv2d_9 = None\n",
      "    dequantize_per_tensor_default_17 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_17, 0.17823737859725952, 0, -128, 127, torch.int8);  quantize_per_tensor_default_17 = None\n",
      "    hardtanh__6 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_17, 0.0, 6.0);  dequantize_per_tensor_default_17 = None\n",
      "    quantize_per_tensor_default_18 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__6, 0.17823737859725952, 0, -128, 127, torch.int8);  hardtanh__6 = None\n",
      "    dequantize_per_tensor_default_18 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_18, 0.17823737859725952, 0, -128, 127, torch.int8);  quantize_per_tensor_default_18 = None\n",
      "    features_4_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_10 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_18, dequantize_per_channel_default_10, features_4_conv_1_0_weight_bias, [2, 2], [1, 1], [1, 1], 144);  dequantize_per_tensor_default_18 = dequantize_per_channel_default_10 = features_4_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_19 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_10, 0.07277931272983551, 3, -128, 127, torch.int8);  conv2d_10 = None\n",
      "    dequantize_per_tensor_default_19 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_19, 0.07277931272983551, 3, -128, 127, torch.int8);  quantize_per_tensor_default_19 = None\n",
      "    hardtanh__7 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_19, 0.0, 6.0);  dequantize_per_tensor_default_19 = None\n",
      "    quantize_per_tensor_default_20 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__7, 0.07277931272983551, 3, -128, 127, torch.int8);  hardtanh__7 = None\n",
      "    dequantize_per_tensor_default_20 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_20, 0.07277931272983551, 3, -128, 127, torch.int8);  quantize_per_tensor_default_20 = None\n",
      "    features_4_conv_2_weight_bias = getattr(getattr(self.features, \"4\").conv, \"2\").weight_bias\n",
      "    conv2d_11 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_20, dequantize_per_channel_default_11, features_4_conv_2_weight_bias);  dequantize_per_tensor_default_20 = dequantize_per_channel_default_11 = features_4_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_21 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_11, 0.16855517029762268, -8, -128, 127, torch.int8);  conv2d_11 = None\n",
      "    dequantize_per_tensor_default_21 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_21, 0.16855517029762268, -8, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_103 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_21, 0.16855517029762268, -8, -128, 127, torch.int8);  quantize_per_tensor_default_21 = None\n",
      "    features_5_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_12 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_103, dequantize_per_channel_default_12, features_5_conv_0_0_weight_bias);  dequantize_per_tensor_default_103 = dequantize_per_channel_default_12 = features_5_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_22 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_12, 0.05896303057670593, 9, -128, 127, torch.int8);  conv2d_12 = None\n",
      "    dequantize_per_tensor_default_22 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_22, 0.05896303057670593, 9, -128, 127, torch.int8);  quantize_per_tensor_default_22 = None\n",
      "    hardtanh__8 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_22, 0.0, 6.0);  dequantize_per_tensor_default_22 = None\n",
      "    quantize_per_tensor_default_23 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__8, 0.05896303057670593, 9, -128, 127, torch.int8);  hardtanh__8 = None\n",
      "    dequantize_per_tensor_default_23 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_23, 0.05896303057670593, 9, -128, 127, torch.int8);  quantize_per_tensor_default_23 = None\n",
      "    features_5_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_13 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_23, dequantize_per_channel_default_13, features_5_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 192);  dequantize_per_tensor_default_23 = dequantize_per_channel_default_13 = features_5_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_24 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_13, 0.06564103811979294, 30, -128, 127, torch.int8);  conv2d_13 = None\n",
      "    dequantize_per_tensor_default_24 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_24, 0.06564103811979294, 30, -128, 127, torch.int8);  quantize_per_tensor_default_24 = None\n",
      "    hardtanh__9 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_24, 0.0, 6.0);  dequantize_per_tensor_default_24 = None\n",
      "    quantize_per_tensor_default_25 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__9, 0.06564103811979294, 30, -128, 127, torch.int8);  hardtanh__9 = None\n",
      "    dequantize_per_tensor_default_25 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_25, 0.06564103811979294, 30, -128, 127, torch.int8);  quantize_per_tensor_default_25 = None\n",
      "    features_5_conv_2_weight_bias = getattr(getattr(self.features, \"5\").conv, \"2\").weight_bias\n",
      "    conv2d_14 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_25, dequantize_per_channel_default_14, features_5_conv_2_weight_bias);  dequantize_per_tensor_default_25 = dequantize_per_channel_default_14 = features_5_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_26 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_14, 0.1602400839328766, -8, -128, 127, torch.int8);  conv2d_14 = None\n",
      "    dequantize_per_tensor_default_26 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_26, 0.1602400839328766, -8, -128, 127, torch.int8);  quantize_per_tensor_default_26 = None\n",
      "    add_1 = torch.ops.aten.add.Tensor(dequantize_per_tensor_default_21, dequantize_per_tensor_default_26);  dequantize_per_tensor_default_21 = dequantize_per_tensor_default_26 = None\n",
      "    quantize_per_tensor_default_27 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_1, 0.1984330117702484, 3, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_27 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_27, 0.1984330117702484, 3, -128, 127, torch.int8);  quantize_per_tensor_default_27 = None\n",
      "    features_6_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_15 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_27, dequantize_per_channel_default_15, features_6_conv_0_0_weight_bias);  dequantize_per_tensor_default_27 = dequantize_per_channel_default_15 = features_6_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_28 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_15, 0.06579098105430603, 36, -128, 127, torch.int8);  conv2d_15 = None\n",
      "    dequantize_per_tensor_default_28 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_28, 0.06579098105430603, 36, -128, 127, torch.int8);  quantize_per_tensor_default_28 = None\n",
      "    hardtanh__10 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_28, 0.0, 6.0);  dequantize_per_tensor_default_28 = None\n",
      "    quantize_per_tensor_default_29 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__10, 0.06579098105430603, 36, -128, 127, torch.int8);  hardtanh__10 = None\n",
      "    dequantize_per_tensor_default_29 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_29, 0.06579098105430603, 36, -128, 127, torch.int8);  quantize_per_tensor_default_29 = None\n",
      "    features_6_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_16 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_29, dequantize_per_channel_default_16, features_6_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 192);  dequantize_per_tensor_default_29 = dequantize_per_channel_default_16 = features_6_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_30 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_16, 0.08226840943098068, 57, -128, 127, torch.int8);  conv2d_16 = None\n",
      "    dequantize_per_tensor_default_30 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_30, 0.08226840943098068, 57, -128, 127, torch.int8);  quantize_per_tensor_default_30 = None\n",
      "    hardtanh__11 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_30, 0.0, 6.0);  dequantize_per_tensor_default_30 = None\n",
      "    quantize_per_tensor_default_31 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__11, 0.08226840943098068, 57, -128, 127, torch.int8);  hardtanh__11 = None\n",
      "    dequantize_per_tensor_default_31 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_31, 0.08226840943098068, 57, -128, 127, torch.int8);  quantize_per_tensor_default_31 = None\n",
      "    features_6_conv_2_weight_bias = getattr(getattr(self.features, \"6\").conv, \"2\").weight_bias\n",
      "    conv2d_17 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_31, dequantize_per_channel_default_17, features_6_conv_2_weight_bias);  dequantize_per_tensor_default_31 = dequantize_per_channel_default_17 = features_6_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_32 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_17, 0.18736989796161652, 50, -128, 127, torch.int8);  conv2d_17 = None\n",
      "    dequantize_per_tensor_default_32 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_32, 0.18736989796161652, 50, -128, 127, torch.int8);  quantize_per_tensor_default_32 = None\n",
      "    add_2 = torch.ops.aten.add.Tensor(add_1, dequantize_per_tensor_default_32);  add_1 = dequantize_per_tensor_default_32 = None\n",
      "    quantize_per_tensor_default_33 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_2, 0.23434363305568695, 9, -128, 127, torch.int8);  add_2 = None\n",
      "    dequantize_per_tensor_default_33 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_33, 0.23434363305568695, 9, -128, 127, torch.int8);  quantize_per_tensor_default_33 = None\n",
      "    features_7_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_18 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_33, dequantize_per_channel_default_18, features_7_conv_0_0_weight_bias);  dequantize_per_tensor_default_33 = dequantize_per_channel_default_18 = features_7_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_34 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_18, 0.07248463481664658, 4, -128, 127, torch.int8);  conv2d_18 = None\n",
      "    dequantize_per_tensor_default_34 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_34, 0.07248463481664658, 4, -128, 127, torch.int8);  quantize_per_tensor_default_34 = None\n",
      "    hardtanh__12 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_34, 0.0, 6.0);  dequantize_per_tensor_default_34 = None\n",
      "    quantize_per_tensor_default_35 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__12, 0.07248463481664658, 4, -128, 127, torch.int8);  hardtanh__12 = None\n",
      "    dequantize_per_tensor_default_35 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_35, 0.07248463481664658, 4, -128, 127, torch.int8);  quantize_per_tensor_default_35 = None\n",
      "    features_7_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_19 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_35, dequantize_per_channel_default_19, features_7_conv_1_0_weight_bias, [2, 2], [1, 1], [1, 1], 192);  dequantize_per_tensor_default_35 = dequantize_per_channel_default_19 = features_7_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_36 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_19, 0.04040756821632385, -40, -128, 127, torch.int8);  conv2d_19 = None\n",
      "    dequantize_per_tensor_default_36 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_36, 0.04040756821632385, -40, -128, 127, torch.int8);  quantize_per_tensor_default_36 = None\n",
      "    hardtanh__13 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_36, 0.0, 6.0);  dequantize_per_tensor_default_36 = None\n",
      "    quantize_per_tensor_default_37 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__13, 0.04040756821632385, -40, -128, 127, torch.int8);  hardtanh__13 = None\n",
      "    dequantize_per_tensor_default_37 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_37, 0.04040756821632385, -40, -128, 127, torch.int8);  quantize_per_tensor_default_37 = None\n",
      "    features_7_conv_2_weight_bias = getattr(getattr(self.features, \"7\").conv, \"2\").weight_bias\n",
      "    conv2d_20 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_37, dequantize_per_channel_default_20, features_7_conv_2_weight_bias);  dequantize_per_tensor_default_37 = dequantize_per_channel_default_20 = features_7_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_38 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_20, 0.1141279935836792, -5, -128, 127, torch.int8);  conv2d_20 = None\n",
      "    dequantize_per_tensor_default_38 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_38, 0.1141279935836792, -5, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_104 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_38, 0.1141279935836792, -5, -128, 127, torch.int8);  quantize_per_tensor_default_38 = None\n",
      "    features_8_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_21 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_104, dequantize_per_channel_default_21, features_8_conv_0_0_weight_bias);  dequantize_per_tensor_default_104 = dequantize_per_channel_default_21 = features_8_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_39 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_21, 0.05610688775777817, 0, -128, 127, torch.int8);  conv2d_21 = None\n",
      "    dequantize_per_tensor_default_39 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_39, 0.05610688775777817, 0, -128, 127, torch.int8);  quantize_per_tensor_default_39 = None\n",
      "    hardtanh__14 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_39, 0.0, 6.0);  dequantize_per_tensor_default_39 = None\n",
      "    quantize_per_tensor_default_40 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__14, 0.05610688775777817, 0, -128, 127, torch.int8);  hardtanh__14 = None\n",
      "    dequantize_per_tensor_default_40 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_40, 0.05610688775777817, 0, -128, 127, torch.int8);  quantize_per_tensor_default_40 = None\n",
      "    features_8_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_22 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_40, dequantize_per_channel_default_22, features_8_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 384);  dequantize_per_tensor_default_40 = dequantize_per_channel_default_22 = features_8_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_41 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_22, 0.1514969766139984, 65, -128, 127, torch.int8);  conv2d_22 = None\n",
      "    dequantize_per_tensor_default_41 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_41, 0.1514969766139984, 65, -128, 127, torch.int8);  quantize_per_tensor_default_41 = None\n",
      "    hardtanh__15 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_41, 0.0, 6.0);  dequantize_per_tensor_default_41 = None\n",
      "    quantize_per_tensor_default_42 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__15, 0.1514969766139984, 65, -128, 127, torch.int8);  hardtanh__15 = None\n",
      "    dequantize_per_tensor_default_42 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_42, 0.1514969766139984, 65, -128, 127, torch.int8);  quantize_per_tensor_default_42 = None\n",
      "    features_8_conv_2_weight_bias = getattr(getattr(self.features, \"8\").conv, \"2\").weight_bias\n",
      "    conv2d_23 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_42, dequantize_per_channel_default_23, features_8_conv_2_weight_bias);  dequantize_per_tensor_default_42 = dequantize_per_channel_default_23 = features_8_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_43 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_23, 0.1292833387851715, -5, -128, 127, torch.int8);  conv2d_23 = None\n",
      "    dequantize_per_tensor_default_43 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_43, 0.1292833387851715, -5, -128, 127, torch.int8);  quantize_per_tensor_default_43 = None\n",
      "    add_3 = torch.ops.aten.add.Tensor(dequantize_per_tensor_default_38, dequantize_per_tensor_default_43);  dequantize_per_tensor_default_38 = dequantize_per_tensor_default_43 = None\n",
      "    quantize_per_tensor_default_44 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_3, 0.13104307651519775, -14, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_44 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_44, 0.13104307651519775, -14, -128, 127, torch.int8);  quantize_per_tensor_default_44 = None\n",
      "    features_9_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_24 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_44, dequantize_per_channel_default_24, features_9_conv_0_0_weight_bias);  dequantize_per_tensor_default_44 = dequantize_per_channel_default_24 = features_9_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_45 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_24, 0.05378320440649986, -8, -128, 127, torch.int8);  conv2d_24 = None\n",
      "    dequantize_per_tensor_default_45 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_45, 0.05378320440649986, -8, -128, 127, torch.int8);  quantize_per_tensor_default_45 = None\n",
      "    hardtanh__16 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_45, 0.0, 6.0);  dequantize_per_tensor_default_45 = None\n",
      "    quantize_per_tensor_default_46 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__16, 0.05378320440649986, -8, -128, 127, torch.int8);  hardtanh__16 = None\n",
      "    dequantize_per_tensor_default_46 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_46, 0.05378320440649986, -8, -128, 127, torch.int8);  quantize_per_tensor_default_46 = None\n",
      "    features_9_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_25 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_46, dequantize_per_channel_default_25, features_9_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 384);  dequantize_per_tensor_default_46 = dequantize_per_channel_default_25 = features_9_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_47 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_25, 0.08111646771430969, 56, -128, 127, torch.int8);  conv2d_25 = None\n",
      "    dequantize_per_tensor_default_47 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_47, 0.08111646771430969, 56, -128, 127, torch.int8);  quantize_per_tensor_default_47 = None\n",
      "    hardtanh__17 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_47, 0.0, 6.0);  dequantize_per_tensor_default_47 = None\n",
      "    quantize_per_tensor_default_48 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__17, 0.08111646771430969, 56, -128, 127, torch.int8);  hardtanh__17 = None\n",
      "    dequantize_per_tensor_default_48 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_48, 0.08111646771430969, 56, -128, 127, torch.int8);  quantize_per_tensor_default_48 = None\n",
      "    features_9_conv_2_weight_bias = getattr(getattr(self.features, \"9\").conv, \"2\").weight_bias\n",
      "    conv2d_26 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_48, dequantize_per_channel_default_26, features_9_conv_2_weight_bias);  dequantize_per_tensor_default_48 = dequantize_per_channel_default_26 = features_9_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_49 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_26, 0.09704110771417618, 9, -128, 127, torch.int8);  conv2d_26 = None\n",
      "    dequantize_per_tensor_default_49 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_49, 0.09704110771417618, 9, -128, 127, torch.int8);  quantize_per_tensor_default_49 = None\n",
      "    add_4 = torch.ops.aten.add.Tensor(add_3, dequantize_per_tensor_default_49);  add_3 = dequantize_per_tensor_default_49 = None\n",
      "    quantize_per_tensor_default_50 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_4, 0.1272606998682022, -11, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_50 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_50, 0.1272606998682022, -11, -128, 127, torch.int8);  quantize_per_tensor_default_50 = None\n",
      "    features_10_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_27 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_50, dequantize_per_channel_default_27, features_10_conv_0_0_weight_bias);  dequantize_per_tensor_default_50 = dequantize_per_channel_default_27 = features_10_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_51 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_27, 0.050029464066028595, 18, -128, 127, torch.int8);  conv2d_27 = None\n",
      "    dequantize_per_tensor_default_51 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_51, 0.050029464066028595, 18, -128, 127, torch.int8);  quantize_per_tensor_default_51 = None\n",
      "    hardtanh__18 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_51, 0.0, 6.0);  dequantize_per_tensor_default_51 = None\n",
      "    quantize_per_tensor_default_52 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__18, 0.050029464066028595, 18, -128, 127, torch.int8);  hardtanh__18 = None\n",
      "    dequantize_per_tensor_default_52 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_52, 0.050029464066028595, 18, -128, 127, torch.int8);  quantize_per_tensor_default_52 = None\n",
      "    features_10_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_28 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_52, dequantize_per_channel_default_28, features_10_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 384);  dequantize_per_tensor_default_52 = dequantize_per_channel_default_28 = features_10_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_53 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_28, 0.09310118108987808, 62, -128, 127, torch.int8);  conv2d_28 = None\n",
      "    dequantize_per_tensor_default_53 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_53, 0.09310118108987808, 62, -128, 127, torch.int8);  quantize_per_tensor_default_53 = None\n",
      "    hardtanh__19 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_53, 0.0, 6.0);  dequantize_per_tensor_default_53 = None\n",
      "    quantize_per_tensor_default_54 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__19, 0.09310118108987808, 62, -128, 127, torch.int8);  hardtanh__19 = None\n",
      "    dequantize_per_tensor_default_54 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_54, 0.09310118108987808, 62, -128, 127, torch.int8);  quantize_per_tensor_default_54 = None\n",
      "    features_10_conv_2_weight_bias = getattr(getattr(self.features, \"10\").conv, \"2\").weight_bias\n",
      "    conv2d_29 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_54, dequantize_per_channel_default_29, features_10_conv_2_weight_bias);  dequantize_per_tensor_default_54 = dequantize_per_channel_default_29 = features_10_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_55 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_29, 0.12139638513326645, -15, -128, 127, torch.int8);  conv2d_29 = None\n",
      "    dequantize_per_tensor_default_55 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_55, 0.12139638513326645, -15, -128, 127, torch.int8);  quantize_per_tensor_default_55 = None\n",
      "    add_5 = torch.ops.aten.add.Tensor(add_4, dequantize_per_tensor_default_55);  add_4 = dequantize_per_tensor_default_55 = None\n",
      "    quantize_per_tensor_default_56 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_5, 0.14760500192642212, -20, -128, 127, torch.int8);  add_5 = None\n",
      "    dequantize_per_tensor_default_56 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_56, 0.14760500192642212, -20, -128, 127, torch.int8);  quantize_per_tensor_default_56 = None\n",
      "    features_11_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_30 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_56, dequantize_per_channel_default_30, features_11_conv_0_0_weight_bias);  dequantize_per_tensor_default_56 = dequantize_per_channel_default_30 = features_11_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_57 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_30, 0.046598777174949646, 10, -128, 127, torch.int8);  conv2d_30 = None\n",
      "    dequantize_per_tensor_default_57 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_57, 0.046598777174949646, 10, -128, 127, torch.int8);  quantize_per_tensor_default_57 = None\n",
      "    hardtanh__20 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_57, 0.0, 6.0);  dequantize_per_tensor_default_57 = None\n",
      "    quantize_per_tensor_default_58 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__20, 0.046598777174949646, 10, -128, 127, torch.int8);  hardtanh__20 = None\n",
      "    dequantize_per_tensor_default_58 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_58, 0.046598777174949646, 10, -128, 127, torch.int8);  quantize_per_tensor_default_58 = None\n",
      "    features_11_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_31 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_58, dequantize_per_channel_default_31, features_11_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 384);  dequantize_per_tensor_default_58 = dequantize_per_channel_default_31 = features_11_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_59 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_31, 0.04767346382141113, 0, -128, 127, torch.int8);  conv2d_31 = None\n",
      "    dequantize_per_tensor_default_59 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_59, 0.04767346382141113, 0, -128, 127, torch.int8);  quantize_per_tensor_default_59 = None\n",
      "    hardtanh__21 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_59, 0.0, 6.0);  dequantize_per_tensor_default_59 = None\n",
      "    quantize_per_tensor_default_60 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__21, 0.04767346382141113, 0, -128, 127, torch.int8);  hardtanh__21 = None\n",
      "    dequantize_per_tensor_default_60 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_60, 0.04767346382141113, 0, -128, 127, torch.int8);  quantize_per_tensor_default_60 = None\n",
      "    features_11_conv_2_weight_bias = getattr(getattr(self.features, \"11\").conv, \"2\").weight_bias\n",
      "    conv2d_32 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_60, dequantize_per_channel_default_32, features_11_conv_2_weight_bias);  dequantize_per_tensor_default_60 = dequantize_per_channel_default_32 = features_11_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_61 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_32, 0.07938653975725174, 4, -128, 127, torch.int8);  conv2d_32 = None\n",
      "    dequantize_per_tensor_default_61 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_61, 0.07938653975725174, 4, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_105 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_61, 0.07938653975725174, 4, -128, 127, torch.int8);  quantize_per_tensor_default_61 = None\n",
      "    features_12_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_33 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_105, dequantize_per_channel_default_33, features_12_conv_0_0_weight_bias);  dequantize_per_tensor_default_105 = dequantize_per_channel_default_33 = features_12_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_62 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_33, 0.028747810050845146, 4, -128, 127, torch.int8);  conv2d_33 = None\n",
      "    dequantize_per_tensor_default_62 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_62, 0.028747810050845146, 4, -128, 127, torch.int8);  quantize_per_tensor_default_62 = None\n",
      "    hardtanh__22 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_62, 0.0, 6.0);  dequantize_per_tensor_default_62 = None\n",
      "    quantize_per_tensor_default_63 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__22, 0.028747810050845146, 4, -128, 127, torch.int8);  hardtanh__22 = None\n",
      "    dequantize_per_tensor_default_63 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_63, 0.028747810050845146, 4, -128, 127, torch.int8);  quantize_per_tensor_default_63 = None\n",
      "    features_12_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_34 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_63, dequantize_per_channel_default_34, features_12_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 576);  dequantize_per_tensor_default_63 = dequantize_per_channel_default_34 = features_12_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_64 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_34, 0.05448530241847038, 22, -128, 127, torch.int8);  conv2d_34 = None\n",
      "    dequantize_per_tensor_default_64 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_64, 0.05448530241847038, 22, -128, 127, torch.int8);  quantize_per_tensor_default_64 = None\n",
      "    hardtanh__23 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_64, 0.0, 6.0);  dequantize_per_tensor_default_64 = None\n",
      "    quantize_per_tensor_default_65 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__23, 0.05448530241847038, 22, -128, 127, torch.int8);  hardtanh__23 = None\n",
      "    dequantize_per_tensor_default_65 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_65, 0.05448530241847038, 22, -128, 127, torch.int8);  quantize_per_tensor_default_65 = None\n",
      "    features_12_conv_2_weight_bias = getattr(getattr(self.features, \"12\").conv, \"2\").weight_bias\n",
      "    conv2d_35 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_65, dequantize_per_channel_default_35, features_12_conv_2_weight_bias);  dequantize_per_tensor_default_65 = dequantize_per_channel_default_35 = features_12_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_66 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_35, 0.05452864617109299, 10, -128, 127, torch.int8);  conv2d_35 = None\n",
      "    dequantize_per_tensor_default_66 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_66, 0.05452864617109299, 10, -128, 127, torch.int8);  quantize_per_tensor_default_66 = None\n",
      "    add_6 = torch.ops.aten.add.Tensor(dequantize_per_tensor_default_61, dequantize_per_tensor_default_66);  dequantize_per_tensor_default_61 = dequantize_per_tensor_default_66 = None\n",
      "    quantize_per_tensor_default_67 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_6, 0.09187797456979752, 6, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_67 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_67, 0.09187797456979752, 6, -128, 127, torch.int8);  quantize_per_tensor_default_67 = None\n",
      "    features_13_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_36 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_67, dequantize_per_channel_default_36, features_13_conv_0_0_weight_bias);  dequantize_per_tensor_default_67 = dequantize_per_channel_default_36 = features_13_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_68 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_36, 0.02842160500586033, 9, -128, 127, torch.int8);  conv2d_36 = None\n",
      "    dequantize_per_tensor_default_68 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_68, 0.02842160500586033, 9, -128, 127, torch.int8);  quantize_per_tensor_default_68 = None\n",
      "    hardtanh__24 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_68, 0.0, 6.0);  dequantize_per_tensor_default_68 = None\n",
      "    quantize_per_tensor_default_69 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__24, 0.02842160500586033, 9, -128, 127, torch.int8);  hardtanh__24 = None\n",
      "    dequantize_per_tensor_default_69 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_69, 0.02842160500586033, 9, -128, 127, torch.int8);  quantize_per_tensor_default_69 = None\n",
      "    features_13_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_37 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_69, dequantize_per_channel_default_37, features_13_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 576);  dequantize_per_tensor_default_69 = dequantize_per_channel_default_37 = features_13_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_70 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_37, 0.06048448756337166, 44, -128, 127, torch.int8);  conv2d_37 = None\n",
      "    dequantize_per_tensor_default_70 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_70, 0.06048448756337166, 44, -128, 127, torch.int8);  quantize_per_tensor_default_70 = None\n",
      "    hardtanh__25 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_70, 0.0, 6.0);  dequantize_per_tensor_default_70 = None\n",
      "    quantize_per_tensor_default_71 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__25, 0.06048448756337166, 44, -128, 127, torch.int8);  hardtanh__25 = None\n",
      "    dequantize_per_tensor_default_71 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_71, 0.06048448756337166, 44, -128, 127, torch.int8);  quantize_per_tensor_default_71 = None\n",
      "    features_13_conv_2_weight_bias = getattr(getattr(self.features, \"13\").conv, \"2\").weight_bias\n",
      "    conv2d_38 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_71, dequantize_per_channel_default_38, features_13_conv_2_weight_bias);  dequantize_per_tensor_default_71 = dequantize_per_channel_default_38 = features_13_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_72 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_38, 0.06608446687459946, 15, -128, 127, torch.int8);  conv2d_38 = None\n",
      "    dequantize_per_tensor_default_72 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_72, 0.06608446687459946, 15, -128, 127, torch.int8);  quantize_per_tensor_default_72 = None\n",
      "    add_7 = torch.ops.aten.add.Tensor(add_6, dequantize_per_tensor_default_72);  add_6 = dequantize_per_tensor_default_72 = None\n",
      "    quantize_per_tensor_default_73 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_7, 0.11169695854187012, 0, -128, 127, torch.int8);  add_7 = None\n",
      "    dequantize_per_tensor_default_73 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_73, 0.11169695854187012, 0, -128, 127, torch.int8);  quantize_per_tensor_default_73 = None\n",
      "    features_14_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_39 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_73, dequantize_per_channel_default_39, features_14_conv_0_0_weight_bias);  dequantize_per_tensor_default_73 = dequantize_per_channel_default_39 = features_14_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_74 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_39, 0.03513330966234207, 13, -128, 127, torch.int8);  conv2d_39 = None\n",
      "    dequantize_per_tensor_default_74 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_74, 0.03513330966234207, 13, -128, 127, torch.int8);  quantize_per_tensor_default_74 = None\n",
      "    hardtanh__26 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_74, 0.0, 6.0);  dequantize_per_tensor_default_74 = None\n",
      "    quantize_per_tensor_default_75 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__26, 0.03513330966234207, 13, -128, 127, torch.int8);  hardtanh__26 = None\n",
      "    dequantize_per_tensor_default_75 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_75, 0.03513330966234207, 13, -128, 127, torch.int8);  quantize_per_tensor_default_75 = None\n",
      "    features_14_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_40 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_75, dequantize_per_channel_default_40, features_14_conv_1_0_weight_bias, [2, 2], [1, 1], [1, 1], 576);  dequantize_per_tensor_default_75 = dequantize_per_channel_default_40 = features_14_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_76 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_40, 0.03605400025844574, -22, -128, 127, torch.int8);  conv2d_40 = None\n",
      "    dequantize_per_tensor_default_76 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_76, 0.03605400025844574, -22, -128, 127, torch.int8);  quantize_per_tensor_default_76 = None\n",
      "    hardtanh__27 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_76, 0.0, 6.0);  dequantize_per_tensor_default_76 = None\n",
      "    quantize_per_tensor_default_77 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__27, 0.03605400025844574, -22, -128, 127, torch.int8);  hardtanh__27 = None\n",
      "    dequantize_per_tensor_default_77 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_77, 0.03605400025844574, -22, -128, 127, torch.int8);  quantize_per_tensor_default_77 = None\n",
      "    features_14_conv_2_weight_bias = getattr(getattr(self.features, \"14\").conv, \"2\").weight_bias\n",
      "    conv2d_41 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_77, dequantize_per_channel_default_41, features_14_conv_2_weight_bias);  dequantize_per_tensor_default_77 = dequantize_per_channel_default_41 = features_14_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_78 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_41, 0.06095806509256363, -3, -128, 127, torch.int8);  conv2d_41 = None\n",
      "    dequantize_per_tensor_default_78 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_78, 0.06095806509256363, -3, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_106 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_78, 0.06095806509256363, -3, -128, 127, torch.int8);  quantize_per_tensor_default_78 = None\n",
      "    features_15_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_42 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_106, dequantize_per_channel_default_42, features_15_conv_0_0_weight_bias);  dequantize_per_tensor_default_106 = dequantize_per_channel_default_42 = features_15_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_79 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_42, 0.027046846225857735, -2, -128, 127, torch.int8);  conv2d_42 = None\n",
      "    dequantize_per_tensor_default_79 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_79, 0.027046846225857735, -2, -128, 127, torch.int8);  quantize_per_tensor_default_79 = None\n",
      "    hardtanh__28 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_79, 0.0, 6.0);  dequantize_per_tensor_default_79 = None\n",
      "    quantize_per_tensor_default_80 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__28, 0.027046846225857735, -2, -128, 127, torch.int8);  hardtanh__28 = None\n",
      "    dequantize_per_tensor_default_80 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_80, 0.027046846225857735, -2, -128, 127, torch.int8);  quantize_per_tensor_default_80 = None\n",
      "    features_15_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_43 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_80, dequantize_per_channel_default_43, features_15_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 960);  dequantize_per_tensor_default_80 = dequantize_per_channel_default_43 = features_15_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_81 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_43, 0.06016433611512184, 20, -128, 127, torch.int8);  conv2d_43 = None\n",
      "    dequantize_per_tensor_default_81 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_81, 0.06016433611512184, 20, -128, 127, torch.int8);  quantize_per_tensor_default_81 = None\n",
      "    hardtanh__29 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_81, 0.0, 6.0);  dequantize_per_tensor_default_81 = None\n",
      "    quantize_per_tensor_default_82 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__29, 0.06016433611512184, 20, -128, 127, torch.int8);  hardtanh__29 = None\n",
      "    dequantize_per_tensor_default_82 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_82, 0.06016433611512184, 20, -128, 127, torch.int8);  quantize_per_tensor_default_82 = None\n",
      "    features_15_conv_2_weight_bias = getattr(getattr(self.features, \"15\").conv, \"2\").weight_bias\n",
      "    conv2d_44 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_82, dequantize_per_channel_default_44, features_15_conv_2_weight_bias);  dequantize_per_tensor_default_82 = dequantize_per_channel_default_44 = features_15_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_83 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_44, 0.09400313347578049, 2, -128, 127, torch.int8);  conv2d_44 = None\n",
      "    dequantize_per_tensor_default_83 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_83, 0.09400313347578049, 2, -128, 127, torch.int8);  quantize_per_tensor_default_83 = None\n",
      "    add_8 = torch.ops.aten.add.Tensor(dequantize_per_tensor_default_78, dequantize_per_tensor_default_83);  dequantize_per_tensor_default_78 = dequantize_per_tensor_default_83 = None\n",
      "    quantize_per_tensor_default_84 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_8, 0.10241571813821793, 23, -128, 127, torch.int8)\n",
      "    dequantize_per_tensor_default_84 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_84, 0.10241571813821793, 23, -128, 127, torch.int8);  quantize_per_tensor_default_84 = None\n",
      "    features_16_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_45 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_84, dequantize_per_channel_default_45, features_16_conv_0_0_weight_bias);  dequantize_per_tensor_default_84 = dequantize_per_channel_default_45 = features_16_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_85 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_45, 0.04639780521392822, -10, -128, 127, torch.int8);  conv2d_45 = None\n",
      "    dequantize_per_tensor_default_85 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_85, 0.04639780521392822, -10, -128, 127, torch.int8);  quantize_per_tensor_default_85 = None\n",
      "    hardtanh__30 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_85, 0.0, 6.0);  dequantize_per_tensor_default_85 = None\n",
      "    quantize_per_tensor_default_86 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__30, 0.04639780521392822, -10, -128, 127, torch.int8);  hardtanh__30 = None\n",
      "    dequantize_per_tensor_default_86 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_86, 0.04639780521392822, -10, -128, 127, torch.int8);  quantize_per_tensor_default_86 = None\n",
      "    features_16_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_46 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_86, dequantize_per_channel_default_46, features_16_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 960);  dequantize_per_tensor_default_86 = dequantize_per_channel_default_46 = features_16_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_87 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_46, 0.08798554539680481, 67, -128, 127, torch.int8);  conv2d_46 = None\n",
      "    dequantize_per_tensor_default_87 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_87, 0.08798554539680481, 67, -128, 127, torch.int8);  quantize_per_tensor_default_87 = None\n",
      "    hardtanh__31 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_87, 0.0, 6.0);  dequantize_per_tensor_default_87 = None\n",
      "    quantize_per_tensor_default_88 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__31, 0.08798554539680481, 67, -128, 127, torch.int8);  hardtanh__31 = None\n",
      "    dequantize_per_tensor_default_88 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_88, 0.08798554539680481, 67, -128, 127, torch.int8);  quantize_per_tensor_default_88 = None\n",
      "    features_16_conv_2_weight_bias = getattr(getattr(self.features, \"16\").conv, \"2\").weight_bias\n",
      "    conv2d_47 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_88, dequantize_per_channel_default_47, features_16_conv_2_weight_bias);  dequantize_per_tensor_default_88 = dequantize_per_channel_default_47 = features_16_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_89 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_47, 0.08014596998691559, -1, -128, 127, torch.int8);  conv2d_47 = None\n",
      "    dequantize_per_tensor_default_89 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_89, 0.08014596998691559, -1, -128, 127, torch.int8);  quantize_per_tensor_default_89 = None\n",
      "    add_9 = torch.ops.aten.add.Tensor(add_8, dequantize_per_tensor_default_89);  add_8 = dequantize_per_tensor_default_89 = None\n",
      "    quantize_per_tensor_default_90 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add_9, 0.15459498763084412, 10, -128, 127, torch.int8);  add_9 = None\n",
      "    dequantize_per_tensor_default_90 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_90, 0.15459498763084412, 10, -128, 127, torch.int8);  quantize_per_tensor_default_90 = None\n",
      "    features_17_conv_0_0_weight_bias = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"0\").weight_bias\n",
      "    conv2d_48 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_90, dequantize_per_channel_default_48, features_17_conv_0_0_weight_bias);  dequantize_per_tensor_default_90 = dequantize_per_channel_default_48 = features_17_conv_0_0_weight_bias = None\n",
      "    quantize_per_tensor_default_91 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_48, 0.047592151910066605, -9, -128, 127, torch.int8);  conv2d_48 = None\n",
      "    dequantize_per_tensor_default_91 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_91, 0.047592151910066605, -9, -128, 127, torch.int8);  quantize_per_tensor_default_91 = None\n",
      "    hardtanh__32 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_91, 0.0, 6.0);  dequantize_per_tensor_default_91 = None\n",
      "    quantize_per_tensor_default_92 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__32, 0.047592151910066605, -9, -128, 127, torch.int8);  hardtanh__32 = None\n",
      "    dequantize_per_tensor_default_92 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_92, 0.047592151910066605, -9, -128, 127, torch.int8);  quantize_per_tensor_default_92 = None\n",
      "    features_17_conv_1_0_weight_bias = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"0\").weight_bias\n",
      "    conv2d_49 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_92, dequantize_per_channel_default_49, features_17_conv_1_0_weight_bias, [1, 1], [1, 1], [1, 1], 960);  dequantize_per_tensor_default_92 = dequantize_per_channel_default_49 = features_17_conv_1_0_weight_bias = None\n",
      "    quantize_per_tensor_default_93 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_49, 0.08663517236709595, 27, -128, 127, torch.int8);  conv2d_49 = None\n",
      "    dequantize_per_tensor_default_93 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_93, 0.08663517236709595, 27, -128, 127, torch.int8);  quantize_per_tensor_default_93 = None\n",
      "    hardtanh__33 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_93, 0.0, 6.0);  dequantize_per_tensor_default_93 = None\n",
      "    quantize_per_tensor_default_94 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__33, 0.08663517236709595, 27, -128, 127, torch.int8);  hardtanh__33 = None\n",
      "    dequantize_per_tensor_default_94 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_94, 0.08663517236709595, 27, -128, 127, torch.int8);  quantize_per_tensor_default_94 = None\n",
      "    features_17_conv_2_weight_bias = getattr(getattr(self.features, \"17\").conv, \"2\").weight_bias\n",
      "    conv2d_50 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_94, dequantize_per_channel_default_50, features_17_conv_2_weight_bias);  dequantize_per_tensor_default_94 = dequantize_per_channel_default_50 = features_17_conv_2_weight_bias = None\n",
      "    quantize_per_tensor_default_95 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_50, 0.05935077369213104, -5, -128, 127, torch.int8);  conv2d_50 = None\n",
      "    dequantize_per_tensor_default_95 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_95, 0.05935077369213104, -5, -128, 127, torch.int8);  quantize_per_tensor_default_95 = None\n",
      "    features_18_0_weight_bias = getattr(getattr(self.features, \"18\"), \"0\").weight_bias\n",
      "    conv2d_51 = torch.ops.aten.conv2d.default(dequantize_per_tensor_default_95, dequantize_per_channel_default_51, features_18_0_weight_bias);  dequantize_per_tensor_default_95 = dequantize_per_channel_default_51 = features_18_0_weight_bias = None\n",
      "    quantize_per_tensor_default_96 = torch.ops.quantized_decomposed.quantize_per_tensor.default(conv2d_51, 0.08606801927089691, 39, -128, 127, torch.int8);  conv2d_51 = None\n",
      "    dequantize_per_tensor_default_96 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_96, 0.08606801927089691, 39, -128, 127, torch.int8);  quantize_per_tensor_default_96 = None\n",
      "    hardtanh__34 = torch.ops.aten.hardtanh_.default(dequantize_per_tensor_default_96, 0.0, 6.0);  dequantize_per_tensor_default_96 = None\n",
      "    quantize_per_tensor_default_97 = torch.ops.quantized_decomposed.quantize_per_tensor.default(hardtanh__34, 0.08606801927089691, 39, -128, 127, torch.int8);  hardtanh__34 = None\n",
      "    dequantize_per_tensor_default_97 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_97, 0.08606801927089691, 39, -128, 127, torch.int8);  quantize_per_tensor_default_97 = None\n",
      "    adaptive_avg_pool2d = torch.ops.aten.adaptive_avg_pool2d.default(dequantize_per_tensor_default_97, [1, 1]);  dequantize_per_tensor_default_97 = None\n",
      "    quantize_per_tensor_default_98 = torch.ops.quantized_decomposed.quantize_per_tensor.default(adaptive_avg_pool2d, 0.08606801927089691, 39, -128, 127, torch.int8);  adaptive_avg_pool2d = None\n",
      "    dequantize_per_tensor_default_98 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_98, 0.08606801927089691, 39, -128, 127, torch.int8);  quantize_per_tensor_default_98 = None\n",
      "    flatten = torch.ops.aten.flatten.using_ints(dequantize_per_tensor_default_98, 1);  dequantize_per_tensor_default_98 = None\n",
      "    quantize_per_tensor_default_99 = torch.ops.quantized_decomposed.quantize_per_tensor.default(flatten, 0.08606801927089691, 39, -128, 127, torch.int8);  flatten = None\n",
      "    dequantize_per_tensor_default_99 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_99, 0.08606801927089691, 39, -128, 127, torch.int8);  quantize_per_tensor_default_99 = None\n",
      "    dropout = torch.ops.aten.dropout.default(dequantize_per_tensor_default_99, 0.2, False);  dequantize_per_tensor_default_99 = None\n",
      "    quantize_per_tensor_default_100 = torch.ops.quantized_decomposed.quantize_per_tensor.default(dropout, 0.014248741790652275, -128, -128, 127, torch.int8);  dropout = None\n",
      "    dequantize_per_tensor_default_100 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_100, 0.014248741790652275, -128, -128, 127, torch.int8);  quantize_per_tensor_default_100 = None\n",
      "    linear = torch.ops.aten.linear.default(dequantize_per_tensor_default_100, dequantize_per_channel_default_52, classifier_1_bias);  dequantize_per_tensor_default_100 = dequantize_per_channel_default_52 = classifier_1_bias = None\n",
      "    quantize_per_tensor_default_101 = torch.ops.quantized_decomposed.quantize_per_tensor.default(linear, 0.022447125986218452, -61, -128, 127, torch.int8);  linear = None\n",
      "    dequantize_per_tensor_default_101 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_101, 0.022447125986218452, -61, -128, 127, torch.int8);  quantize_per_tensor_default_101 = None\n",
      "    return pytree.tree_unflatten((dequantize_per_tensor_default_101,), self._out_spec)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1757303413.778856    6423 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1757303413.778882    6423 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "I0000 00:00:1757303413.779160    6423 reader.cc:83] Reading SavedModel from: /tmp/tmp6ohyr2g1\n",
      "I0000 00:00:1757303413.783841    6423 reader.cc:52] Reading meta graph with tags { serve }\n",
      "I0000 00:00:1757303413.783866    6423 reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp6ohyr2g1\n",
      "I0000 00:00:1757303413.820058    6423 loader.cc:236] Restoring SavedModel bundle.\n",
      "I0000 00:00:1757303414.158779    6423 loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp6ohyr2g1\n",
      "I0000 00:00:1757303414.245415    6423 loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 466282 microseconds.\n",
      "I0000 00:00:1757303415.540812    6423 flatbuffer_export.cc:4150] Estimated count of arithmetic ops: 608.445 M  ops, equivalently 304.223 M  MACs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "# PT2E (torch.ao)\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
    "\n",
    "# AI Edge Torch\n",
    "import ai_edge_torch as aet\n",
    "from ai_edge_torch.quantize import pt2e_quantizer as aet_q\n",
    "from ai_edge_torch.quantize import quant_config as aet_qc\n",
    "\n",
    "m = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).eval()\n",
    "example_inputs = (torch.randn(1,3,224,224),)\n",
    "\n",
    "# 1) Capture to ExportedProgram (ATen graph)\n",
    "ep = torch.export.export(m, example_inputs).module()  # 2.6+ API\n",
    "\n",
    "# 2) Configure an AET PT2E quantizer (symmetric, per-channel)\n",
    "qspec = aet_q.get_symmetric_quantization_config(is_per_channel=True)\n",
    "quantizer = aet_q.PT2EQuantizer().set_global(qspec)\n",
    "\n",
    "# 3) Prepare + calibrate\n",
    "prepared = prepare_pt2e(ep, quantizer)\n",
    "with torch.no_grad():\n",
    "    for _ in range(32): prepared(torch.randn(1,3,224,224))\n",
    "\n",
    "# 4) Convert (keep Q/DQ explicit for TFLite lowering)\n",
    "quantized = convert_pt2e(prepared, fold_quantize=False)\n",
    "\n",
    "print(quantized)\n",
    "\n",
    "# 5) Convert to TFLite\n",
    "edge_model = aet.convert(\n",
    "    quantized,\n",
    "    example_inputs,\n",
    "    quant_config=aet_qc.QuantConfig(pt2e_quantizer=quantizer),\n",
    ")\n",
    "edge_model.export(\"mobilenetv2_int8.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf000232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': np.float32(0.011216138), 'cosine': np.float32(0.9931354), 'top1_agree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) Prepare one test input (use real, normalized data if possible)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    y_fp = m.eval()(x).cpu().numpy()\n",
    "\n",
    "# 2) Run TFLite\n",
    "import tensorflow as tf  # or tflite_runtime.interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=\"mobilenetv2_int8.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "inp = interpreter.get_input_details()[0]\n",
    "out = interpreter.get_output_details()[0]\n",
    "\n",
    "x_np = x.cpu().numpy().astype(np.float32)\n",
    "\n",
    "# Handle quantized or float I/O automatically\n",
    "def set_input(interpreter, detail, x_float):\n",
    "    if np.issubdtype(detail[\"dtype\"], np.floating):\n",
    "        interpreter.set_tensor(detail[\"index\"], x_float)\n",
    "    else:\n",
    "        scale, zero = detail[\"quantization\"]\n",
    "        x_q = np.round(x_float / scale + zero)\n",
    "        qmin = np.iinfo(detail[\"dtype\"]).min\n",
    "        qmax = np.iinfo(detail[\"dtype\"]).max\n",
    "        x_q = np.clip(x_q, qmin, qmax).astype(detail[\"dtype\"])\n",
    "        interpreter.set_tensor(detail[\"index\"], x_q)\n",
    "\n",
    "def get_output(interpreter, detail):\n",
    "    y = interpreter.get_tensor(detail[\"index\"])\n",
    "    if not np.issubdtype(detail[\"dtype\"], np.floating):\n",
    "        scale, zero = detail[\"quantization\"]\n",
    "        y = (y.astype(np.float32) - zero) * scale\n",
    "    return y\n",
    "\n",
    "set_input(interpreter, inp, x_np)\n",
    "interpreter.invoke()\n",
    "y_tfl = get_output(interpreter, out)\n",
    "\n",
    "# 3) Metrics (PyTorch vs TFLite)\n",
    "mse = np.mean((y_fp - y_tfl) ** 2)\n",
    "cos = np.mean(np.sum(y_fp * y_tfl, axis=1) /\n",
    "              (np.linalg.norm(y_fp, axis=1) * np.linalg.norm(y_tfl, axis=1) + 1e-12))\n",
    "top1_pt  = y_fp.argmax(axis=1)\n",
    "top1_tfl = y_tfl.argmax(axis=1)\n",
    "agree = float((top1_pt == top1_tfl).mean())\n",
    "\n",
    "print({\"mse\": mse, \"cosine\": cos, \"top1_agree\": agree})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
